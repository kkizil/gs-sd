cd C:\Users\Uygur\Documents\NetBeansProjects\AdvOptHW4; "JAVA_HOME=C:\\Program Files\\Java\\jdk-13.0.1" cmd /c "\"\"C:\\Program Files\\NetBeans-11.2\\netbeans\\java\\maven\\bin\\mvn.cmd\" -Dexec.args=\"-classpath %classpath edu.advopthw4.Main\" -Dexec.executable=\"C:\\Program Files\\Java\\jdk-13.0.1\\bin\\java.exe\" -Dmaven.ext.class.path=\"C:\\Program Files\\NetBeans-11.2\\netbeans\\java\\maven-nblib\\netbeans-eventspy.jar\" -Dfile.encoding=UTF-8 process-classes org.codehaus.mojo:exec-maven-plugin:1.5.0:exec\""
Scanning for projects...
                                                                        
------------------------------------------------------------------------
Building AdvOptHW4 Version.1.0
------------------------------------------------------------------------

--- maven-resources-plugin:2.6:resources (default-resources) @ AdvOptHW4 ---
Using 'UTF-8' encoding to copy filtered resources.
skip non existing resourceDirectory C:\Users\Uygur\Documents\NetBeansProjects\AdvOptHW4\src\main\resources

--- maven-compiler-plugin:3.1:compile (default-compile) @ AdvOptHW4 ---
Changes detected - recompiling the module!
Compiling 1 source file to C:\Users\Uygur\Documents\NetBeansProjects\AdvOptHW4\target\classes

--- exec-maven-plugin:1.5.0:exec (default-cli) @ AdvOptHW4 ---
Initial f(x) is: 292.0
Iteration 0
Gradient at x = (6,000000, 6,000000) is: (-244,000375, -23,999291).
Optimal stepsize is between: [0,010301, 0,010371]
f(x + d*a) = 69,188005
x + d*a = [3,486672, 5,752795]
f(x + d*b) = 69,187621
x + d*b = [3,469418, 5,751098]
***************************************
Iteration 1
Gradient at x = (3,486672, 5,752795) is: (2,896172, -32,075320).
Optimal stepsize is between: [0,111895, 0,111966]
f(x + d*a) = 11,017361
x + d*a = [3,810740, 2,163716]
f(x + d*b) = 11,017348
x + d*b = [3,810945, 2,161448]
***************************************
Iteration 2
Gradient at x = (3,810740, 2,163716) is: (-22,714630, -2,066614).
Optimal stepsize is between: [0,043748, 0,043792]
f(x + d*a) = 2,213398
x + d*a = [2,817019, 2,073306]
f(x + d*b) = 2,213397
x + d*b = [2,816026, 2,073215]
***************************************
Iteration 3
Gradient at x = (2,817019, 2,073306) is: (0,477707, -5,318368).
Optimal stepsize is between: [0,114477, 0,114547]
f(x + d*a) = 0,580679
x + d*a = [2,871705, 1,464476]
f(x + d*b) = 0,580679
x + d*b = [2,871739, 1,464100]
***************************************
Iteration 4
Gradient at x = (2,871705, 1,464476) is: (-2,535028, -0,228983).
Optimal stepsize is between: [0,138400, 0,138470]
f(x + d*a) = 0,192426
x + d*a = [2,520858, 1,432785]
f(x + d*b) = 0,192426
x + d*b = [2,520679, 1,432769]
***************************************
Iteration 5
Gradient at x = (2,520858, 1,432785) is: (0,124206, -1,378847).
Optimal stepsize is between: [0,115005, 0,115049]
f(x + d*a) = 0,082188
x + d*a = [2,535142, 1,274210]
f(x + d*b) = 0,082188
x + d*b = [2,535148, 1,274150]
***************************************
Iteration 6
Gradient at x = (2,535142, 1,274210) is: (-0,586454, -0,053114).
Optimal stepsize is between: [0,258815, 0,258859]
f(x + d*a) = 0,040523
x + d*a = [2,383359, 1,260464]
f(x + d*b) = 0,040523
x + d*b = [2,383333, 1,260461]
***************************************
Iteration 7
Gradient at x = (2,383359, 1,260464) is: (0,049776, -0,550273).
Optimal stepsize is between: [0,115120, 0,115190]
f(x + d*a) = 0,022946
x + d*a = [2,389089, 1,197117]
f(x + d*b) = 0,022946
x + d*b = [2,389093, 1,197078]
***************************************
Iteration 8
Gradient at x = (2,389089, 1,197117) is: (-0,225330, -0,020575).
Optimal stepsize is between: [0,360040, 0,360110]
f(x + d*a) = 0,014101
x + d*a = [2,307962, 1,189709]
f(x + d*b) = 0,014101
x + d*b = [2,307946, 1,189707]
***************************************
Iteration 9
Gradient at x = (2,307962, 1,189709) is: (0,026083, -0,285824).
Optimal stepsize is between: [0,115120, 0,115190]
f(x + d*a) = 0,009358
x + d*a = [2,310964, 1,156805]
f(x + d*b) = 0,009358
x + d*b = [2,310966, 1,156785]
***************************************
Iteration 10
Gradient at x = (2,310964, 1,156805) is: (-0,114989, -0,010582).
Optimal stepsize is between: [0,435588, 0,435631]
f(x + d*a) = 0,006525
x + d*a = [2,260877, 1,152195]
f(x + d*b) = 0,006525
x + d*b = [2,260872, 1,152195]
***************************************
Iteration 11
Gradient at x = (2,260877, 1,152195) is: (0,016011, -0,174057).
Optimal stepsize is between: [0,115076, 0,115147]
f(x + d*a) = 0,004767
x + d*a = [2,262719, 1,132166]
f(x + d*b) = 0,004767
x + d*b = [2,262720, 1,132153]
***************************************
Iteration 12
Gradient at x = (2,262719, 1,132166) is: (-0,069308, -0,006449).
Optimal stepsize is between: [0,491918, 0,491989]
f(x + d*a) = 0,003594
x + d*a = [2,228625, 1,128993]
f(x + d*b) = 0,003594
x + d*b = [2,228620, 1,128993]
***************************************
Iteration 13
Gradient at x = (2,228625, 1,128993) is: (0,010923, -0,117446).
Optimal stepsize is between: [0,115005, 0,115076]
f(x + d*a) = 0,002794
x + d*a = [2,229881, 1,115486]
f(x + d*b) = 0,002794
x + d*b = [2,229882, 1,115478]
***************************************
Iteration 14
Gradient at x = (2,229881, 1,115486) is: (-0,046410, -0,004366).
Optimal stepsize is between: [0,534803, 0,534847]
f(x + d*a) = 0,002219
x + d*a = [2,205061, 1,113151]
f(x + d*b) = 0,002219
x + d*b = [2,205059, 1,113151]
***************************************
Iteration 15
Gradient at x = (2,205061, 1,113151) is: (0,007991, -0,084965).
Optimal stepsize is between: [0,114934, 0,115005]
f(x + d*a) = 0,001801
x + d*a = [2,205980, 1,103386]
f(x + d*b) = 0,001801
x + d*b = [2,205980, 1,103380]
***************************************
Iteration 16
Gradient at x = (2,205980, 1,103386) is: (-0,033373, -0,003168).
Optimal stepsize is between: [0,567978, 0,568049]
f(x + d*a) = 0,001484
x + d*a = [2,187025, 1,101587]
f(x + d*b) = 0,001484
x + d*b = [2,187022, 1,101586]
***************************************
Iteration 17
Gradient at x = (2,187025, 1,101587) is: (0,006130, -0,064594).
Optimal stepsize is between: [0,114891, 0,114934]
f(x + d*a) = 0,001242
x + d*a = [2,187729, 1,094165]
f(x + d*b) = 0,001242
x + d*b = [2,187729, 1,094162]
***************************************
Iteration 18
Gradient at x = (2,187729, 1,094165) is: (-0,025260, -0,002407).
Optimal stepsize is between: [0,593399, 0,593469]
f(x + d*a) = 0,001053
x + d*a = [2,172739, 1,092737]
f(x + d*b) = 0,001053
x + d*b = [2,172738, 1,092737]
***************************************
Iteration 19
Gradient at x = (2,172739, 1,092737) is: (0,004852, -0,050938).
Optimal stepsize is between: [0,114847, 0,114918]
f(x + d*a) = 0,000902
x + d*a = [2,173297, 1,086887]
f(x + d*b) = 0,000902
x + d*b = [2,173297, 1,086883]
***************************************
Iteration 20
Gradient at x = (2,173297, 1,086887) is: (-0,019863, -0,001909).
Optimal stepsize is between: [0,615056, 0,615127]
f(x + d*a) = 0,000780
x + d*a = [2,161080, 1,085713]
f(x + d*b) = 0,000780
x + d*b = [2,161078, 1,085713]
***************************************
Iteration 21
Gradient at x = (2,161080, 1,085713) is: (0,003975, -0,041386).
Optimal stepsize is between: [0,114776, 0,114847]
f(x + d*a) = 0,000681
x + d*a = [2,161536, 1,080963]
f(x + d*b) = 0,000681
x + d*b = [2,161536, 1,080960]
***************************************
Iteration 22
Gradient at x = (2,161536, 1,080963) is: (-0,016080, -0,001560).
Optimal stepsize is between: [0,633631, 0,633702]
f(x + d*a) = 0,000599
x + d*a = [2,151347, 1,079974]
f(x + d*b) = 0,000599
x + d*b = [2,151346, 1,079974]
***************************************
Iteration 23
Gradient at x = (2,151347, 1,079974) is: (0,003337, -0,034408).
Optimal stepsize is between: [0,114706, 0,114749]
f(x + d*a) = 0,000530
x + d*a = [2,151730, 1,076028]
f(x + d*b) = 0,000530
x + d*b = [2,151730, 1,076026]
***************************************
Iteration 24
Gradient at x = (2,151730, 1,076028) is: (-0,013321, -0,001302).
Optimal stepsize is between: [0,649157, 0,649228]
f(x + d*a) = 0,000472
x + d*a = [2,143082, 1,075182]
f(x + d*b) = 0,000472
x + d*b = [2,143081, 1,075182]
***************************************
Iteration 25
Gradient at x = (2,143082, 1,075182) is: (0,002847, -0,029128).
Optimal stepsize is between: [0,114635, 0,114706]
f(x + d*a) = 0,000423
x + d*a = [2,143408, 1,071843]
f(x + d*b) = 0,000423
x + d*b = [2,143409, 1,071841]
***************************************
Iteration 26
Gradient at x = (2,143408, 1,071843) is: (-0,011242, -0,001110).
Optimal stepsize is between: [0,663202, 0,663272]
f(x + d*a) = 0,000381
x + d*a = [2,135952, 1,071107]
f(x + d*b) = 0,000381
x + d*b = [2,135952, 1,071107]
***************************************
Iteration 27
Gradient at x = (2,135952, 1,071107) is: (0,002472, -0,025046).
Optimal stepsize is between: [0,114547, 0,114618]
f(x + d*a) = 0,000345
x + d*a = [2,136236, 1,068238]
f(x + d*b) = 0,000345
x + d*b = [2,136236, 1,068236]
***************************************
Iteration 28
Gradient at x = (2,136236, 1,068238) is: (-0,009633, -0,000962).
Optimal stepsize is between: [0,676331, 0,676402]
f(x + d*a) = 0,000313
x + d*a = [2,129720, 1,067587]
f(x + d*b) = 0,000313
x + d*b = [2,129720, 1,067587]
***************************************
Iteration 29
Gradient at x = (2,129720, 1,067587) is: (0,002178, -0,021819).
Optimal stepsize is between: [0,114477, 0,114547]
f(x + d*a) = 0,000285
x + d*a = [2,129970, 1,065090]
f(x + d*b) = 0,000285
x + d*b = [2,129970, 1,065088]
***************************************
Iteration 30
Gradient at x = (2,129970, 1,065090) is: (-0,008362, -0,000840).
Optimal stepsize is between: [0,686482, 0,686553]
f(x + d*a) = 0,000261
x + d*a = [2,124229, 1,064513]
f(x + d*b) = 0,000261
x + d*b = [2,124229, 1,064513]
***************************************
Iteration 31
Gradient at x = (2,124229, 1,064513) is: (0,001926, -0,019190).
Optimal stepsize is between: [0,114406, 0,114477]
f(x + d*a) = 0,000240
x + d*a = [2,124450, 1,062318]
f(x + d*b) = 0,000240
x + d*b = [2,124450, 1,062317]
***************************************
Iteration 32
Gradient at x = (2,124450, 1,062318) is: (-0,007337, -0,000745).
Optimal stepsize is between: [0,697390, 0,697460]
f(x + d*a) = 0,000221
x + d*a = [2,119333, 1,061798]
f(x + d*b) = 0,000221
x + d*b = [2,119332, 1,061798]
***************************************
Iteration 33
Gradient at x = (2,119333, 1,061798) is: (0,001731, -0,017057).
Optimal stepsize is between: [0,114335, 0,114379]
f(x + d*a) = 0,000204
x + d*a = [2,119531, 1,059848]
f(x + d*b) = 0,000204
x + d*b = [2,119531, 1,059847]
***************************************
Iteration 34
Gradient at x = (2,119531, 1,059848) is: (-0,006499, -0,000664).
Optimal stepsize is between: [0,705743, 0,705814]
f(x + d*a) = 0,000189
x + d*a = [2,114944, 1,059380]
f(x + d*b) = 0,000189
x + d*b = [2,114943, 1,059380]
***************************************
Iteration 35
Gradient at x = (2,114944, 1,059380) is: (0,001558, -0,015265).
Optimal stepsize is between: [0,114292, 0,114335]
f(x + d*a) = 0,000176
x + d*a = [2,115122, 1,057635]
f(x + d*b) = 0,000176
x + d*b = [2,115122, 1,057635]
***************************************
Iteration 36
Gradient at x = (2,115122, 1,057635) is: (-0,005805, -0,000595).
Optimal stepsize is between: [0,713144, 0,713188]
f(x + d*a) = 0,000164
x + d*a = [2,110982, 1,057211]
f(x + d*b) = 0,000164
x + d*b = [2,110982, 1,057211]
***************************************
Iteration 37
Gradient at x = (2,110982, 1,057211) is: (0,001411, -0,013758).
Optimal stepsize is between: [0,114248, 0,114319]
f(x + d*a) = 0,000153
x + d*a = [2,111143, 1,055639]
f(x + d*b) = 0,000153
x + d*b = [2,111143, 1,055638]
***************************************
Iteration 38
Gradient at x = (2,111143, 1,055639) is: (-0,005222, -0,000539).
Optimal stepsize is between: [0,720115, 0,720185]
f(x + d*a) = 0,000143
x + d*a = [2,107382, 1,055251]
f(x + d*b) = 0,000143
x + d*b = [2,107382, 1,055251]
***************************************
Iteration 39
Gradient at x = (2,107382, 1,055251) is: (0,001287, -0,012479).
Optimal stepsize is between: [0,114177, 0,114248]
f(x + d*a) = 0,000134
x + d*a = [2,107529, 1,053826]
f(x + d*b) = 0,000134
x + d*b = [2,107529, 1,053825]
***************************************
Iteration 40
Gradient at x = (2,107529, 1,053826) is: (-0,004727, -0,000493).
Optimal stepsize is between: [0,728697, 0,728768]
f(x + d*a) = 0,000125
x + d*a = [2,104085, 1,053467]
f(x + d*b) = 0,000125
x + d*b = [2,104084, 1,053467]
***************************************
Iteration 41
Gradient at x = (2,104085, 1,053467) is: (0,001188, -0,011397).
Optimal stepsize is between: [0,114106, 0,114150]
f(x + d*a) = 0,000118
x + d*a = [2,104220, 1,052167]
f(x + d*b) = 0,000118
x + d*b = [2,104220, 1,052166]
***************************************
Iteration 42
Gradient at x = (2,104220, 1,052167) is: (-0,004303, -0,000451).
Optimal stepsize is between: [0,734758, 0,734802]
f(x + d*a) = 0,000111
x + d*a = [2,101059, 1,051835]
f(x + d*b) = 0,000111
x + d*b = [2,101059, 1,051835]
***************************************
Iteration 43
Gradient at x = (2,101059, 1,051835) is: (0,001095, -0,010446).
Optimal stepsize is between: [0,114063, 0,114134]
f(x + d*a) = 0,000105
x + d*a = [2,101184, 1,050644]
f(x + d*b) = 0,000105
x + d*b = [2,101184, 1,050643]
***************************************
Iteration 44
Gradient at x = (2,101184, 1,050644) is: (-0,003936, -0,000415).
Optimal stepsize is between: [0,740433, 0,740476]
f(x + d*a) = 0,000099
x + d*a = [2,098269, 1,050337]
f(x + d*b) = 0,000099
x + d*b = [2,098269, 1,050337]
***************************************
Iteration 45
Gradient at x = (2,098269, 1,050337) is: (0,001013, -0,009617).
Optimal stepsize is between: [0,113992, 0,114063]
f(x + d*a) = 0,000094
x + d*a = [2,098385, 1,049240]
f(x + d*b) = 0,000094
x + d*b = [2,098385, 1,049240]
***************************************
Iteration 46
Gradient at x = (2,098385, 1,049240) is: (-0,003617, -0,000385).
Optimal stepsize is between: [0,748073, 0,748143]
f(x + d*a) = 0,000089
x + d*a = [2,095679, 1,048952]
f(x + d*b) = 0,000089
x + d*b = [2,095679, 1,048952]
***************************************
Iteration 47
Gradient at x = (2,095679, 1,048952) is: (0,000948, -0,008903).
Optimal stepsize is between: [0,113921, 0,113965]
f(x + d*a) = 0,000084
x + d*a = [2,095787, 1,047938]
f(x + d*b) = 0,000084
x + d*b = [2,095787, 1,047938]
***************************************
Iteration 48
Gradient at x = (2,095787, 1,047938) is: (-0,003337, -0,000357).
Optimal stepsize is between: [0,753306, 0,753377]
f(x + d*a) = 0,000080
x + d*a = [2,093273, 1,047669]
f(x + d*b) = 0,000080
x + d*b = [2,093273, 1,047669]
***************************************
Iteration 49
Gradient at x = (2,093273, 1,047669) is: (0,000884, -0,008259).
Optimal stepsize is between: [0,113878, 0,113948]
f(x + d*a) = 0,000076
x + d*a = [2,093374, 1,046729]
f(x + d*b) = 0,000076
x + d*b = [2,093374, 1,046728]
***************************************
Iteration 50
Gradient at x = (2,093374, 1,046729) is: (-0,003090, -0,000332).
Optimal stepsize is between: [0,758126, 0,758169]
f(x + d*a) = 0,000072
x + d*a = [2,091031, 1,046477]
f(x + d*b) = 0,000072
x + d*b = [2,091031, 1,046477]
***************************************
Iteration 51
Gradient at x = (2,091031, 1,046477) is: (0,000827, -0,007688).
Optimal stepsize is between: [0,113807, 0,113878]
f(x + d*a) = 0,000069
x + d*a = [2,091125, 1,045602]
f(x + d*b) = 0,000069
x + d*b = [2,091125, 1,045601]
***************************************
Iteration 52
Gradient at x = (2,091125, 1,045602) is: (-0,002871, -0,000312).
Optimal stepsize is between: [0,765325, 0,765369]
f(x + d*a) = 0,000066
x + d*a = [2,088928, 1,045363]
f(x + d*b) = 0,000066
x + d*b = [2,088928, 1,045363]
***************************************
Iteration 53
Gradient at x = (2,088928, 1,045363) is: (0,000782, -0,007189).
Optimal stepsize is between: [0,113736, 0,113780]
f(x + d*a) = 0,000063
x + d*a = [2,089017, 1,044545]
f(x + d*b) = 0,000063
x + d*b = [2,089017, 1,044545]
***************************************
Iteration 54
Gradient at x = (2,089017, 1,044545) is: (-0,002675, -0,000292).
Optimal stepsize is between: [0,769801, 0,769872]
f(x + d*a) = 0,000060
x + d*a = [2,086958, 1,044320]
f(x + d*b) = 0,000060
x + d*b = [2,086957, 1,044320]
***************************************
Iteration 55
Gradient at x = (2,086958, 1,044320) is: (0,000735, -0,006730).
Optimal stepsize is between: [0,113693, 0,113763]
f(x + d*a) = 0,000057
x + d*a = [2,087041, 1,043555]
f(x + d*b) = 0,000057
x + d*b = [2,087041, 1,043554]
***************************************
Iteration 56
Gradient at x = (2,087041, 1,043555) is: (-0,002500, -0,000275).
Optimal stepsize is between: [0,774321, 0,774392]
f(x + d*a) = 0,000055
x + d*a = [2,085105, 1,043342]
f(x + d*b) = 0,000055
x + d*b = [2,085105, 1,043342]
***************************************
Iteration 57
Gradient at x = (2,085105, 1,043342) is: (0,000694, -0,006319).
Optimal stepsize is between: [0,113622, 0,113693]
f(x + d*a) = 0,000053
x + d*a = [2,085184, 1,042624]
f(x + d*b) = 0,000053
x + d*b = [2,085184, 1,042624]
***************************************
Iteration 58
Gradient at x = (2,085184, 1,042624) is: (-0,002343, -0,000260).
Optimal stepsize is between: [0,780992, 0,781063]
f(x + d*a) = 0,000050
x + d*a = [2,083354, 1,042421]
f(x + d*b) = 0,000050
x + d*b = [2,083354, 1,042421]
***************************************
Iteration 59
Gradient at x = (2,083354, 1,042421) is: (0,000660, -0,005954).
Optimal stepsize is between: [0,113551, 0,113595]
f(x + d*a) = 0,000048
x + d*a = [2,083429, 1,041745]
f(x + d*b) = 0,000048
x + d*b = [2,083429, 1,041745]
***************************************
Iteration 60
Gradient at x = (2,083429, 1,041745) is: (-0,002200, -0,000245).
Optimal stepsize is between: [0,785283, 0,785354]
f(x + d*a) = 0,000047
x + d*a = [2,081702, 1,041553]
f(x + d*b) = 0,000047
x + d*b = [2,081701, 1,041553]
***************************************
Iteration 61
Gradient at x = (2,081702, 1,041553) is: (0,000626, -0,005615).
Optimal stepsize is between: [0,113507, 0,113551]
f(x + d*a) = 0,000045
x + d*a = [2,081773, 1,040915]
f(x + d*b) = 0,000045
x + d*b = [2,081773, 1,040915]
***************************************
Iteration 62
Gradient at x = (2,081773, 1,040915) is: (-0,002071, -0,000232).
Optimal stepsize is between: [0,789248, 0,789319]
f(x + d*a) = 0,000043
x + d*a = [2,080138, 1,040732]
f(x + d*b) = 0,000043
x + d*b = [2,080138, 1,040732]
***************************************
Iteration 63
Gradient at x = (2,080138, 1,040732) is: (0,000594, -0,005306).
Optimal stepsize is between: [0,113464, 0,113534]
f(x + d*a) = 0,000041
x + d*a = [2,080205, 1,040130]
f(x + d*b) = 0,000041
x + d*b = [2,080205, 1,040130]
***************************************
Iteration 64
Gradient at x = (2,080205, 1,040130) is: (-0,001954, -0,000220).
Optimal stepsize is between: [0,793310, 0,793381]
f(x + d*a) = 0,000040
x + d*a = [2,078655, 1,039956]
f(x + d*b) = 0,000040
x + d*b = [2,078655, 1,039956]
***************************************
Iteration 65
Gradient at x = (2,078655, 1,039956) is: (0,000566, -0,005024).
Optimal stepsize is between: [0,113393, 0,113464]
f(x + d*a) = 0,000038
x + d*a = [2,078720, 1,039386]
f(x + d*b) = 0,000038
x + d*b = [2,078720, 1,039386]
***************************************
Iteration 66
Gradient at x = (2,078720, 1,039386) is: (-0,001846, -0,000210).
Optimal stepsize is between: [0,799742, 0,799812]
f(x + d*a) = 0,000037
x + d*a = [2,077243, 1,039218]
f(x + d*b) = 0,000037
x + d*b = [2,077243, 1,039218]
***************************************
Iteration 67
Gradient at x = (2,077243, 1,039218) is: (0,000543, -0,004772).
Optimal stepsize is between: [0,113322, 0,113366]
f(x + d*a) = 0,000036
x + d*a = [2,077305, 1,038677]
f(x + d*b) = 0,000036
x + d*b = [2,077305, 1,038677]
***************************************
Iteration 68
Gradient at x = (2,077305, 1,038677) is: (-0,001748, -0,000200).
Optimal stepsize is between: [0,803733, 0,803777]
f(x + d*a) = 0,000034
x + d*a = [2,075900, 1,038517]
f(x + d*b) = 0,000034
x + d*b = [2,075900, 1,038517]
***************************************
Iteration 69
Gradient at x = (2,075900, 1,038517) is: (0,000518, -0,004535).
Optimal stepsize is between: [0,113252, 0,113322]
f(x + d*a) = 0,000033
x + d*a = [2,075958, 1,038003]
f(x + d*b) = 0,000033
x + d*b = [2,075958, 1,038003]
***************************************
Iteration 70
Gradient at x = (2,075958, 1,038003) is: (-0,001657, -0,000191).
Optimal stepsize is between: [0,810078, 0,810121]
f(x + d*a) = 0,000032
x + d*a = [2,074616, 1,037848]
f(x + d*b) = 0,000032
x + d*b = [2,074616, 1,037848]
***************************************
Iteration 71
Gradient at x = (2,074616, 1,037848) is: (0,000499, -0,004321).
Optimal stepsize is between: [0,113164, 0,113235]
f(x + d*a) = 0,000031
x + d*a = [2,074672, 1,037359]
f(x + d*b) = 0,000031
x + d*b = [2,074672, 1,037359]
***************************************
Iteration 72
Gradient at x = (2,074672, 1,037359) is: (-0,001574, -0,000183).
Optimal stepsize is between: [0,815708, 0,815779]
f(x + d*a) = 0,000030
x + d*a = [2,073388, 1,037210]
f(x + d*b) = 0,000030
x + d*b = [2,073388, 1,037209]
***************************************
Iteration 73
Gradient at x = (2,073388, 1,037210) is: (0,000480, -0,004122).
Optimal stepsize is between: [0,113093, 0,113164]
f(x + d*a) = 0,000029
x + d*a = [2,073443, 1,036743]
f(x + d*b) = 0,000029
x + d*b = [2,073443, 1,036743]
***************************************
Iteration 74
Gradient at x = (2,073443, 1,036743) is: (-0,001497, -0,000175).
Optimal stepsize is between: [0,820343, 0,820413]
f(x + d*a) = 0,000028
x + d*a = [2,072215, 1,036599]
f(x + d*b) = 0,000028
x + d*b = [2,072215, 1,036599]
***************************************
Iteration 75
Gradient at x = (2,072215, 1,036599) is: (0,000461, -0,003935).
Optimal stepsize is between: [0,113023, 0,113093]
f(x + d*a) = 0,000027
x + d*a = [2,072267, 1,036155]
f(x + d*b) = 0,000027
x + d*b = [2,072267, 1,036154]
***************************************
Iteration 76
Gradient at x = (2,072267, 1,036155) is: (-0,001425, -0,000169).
Optimal stepsize is between: [0,825930, 0,826001]
f(x + d*a) = 0,000026
x + d*a = [2,071090, 1,036015]
f(x + d*b) = 0,000026
x + d*b = [2,071090, 1,036015]
***************************************
Iteration 77
Gradient at x = (2,071090, 1,036015) is: (0,000445, -0,003764).
Optimal stepsize is between: [0,112952, 0,112996]
f(x + d*a) = 0,000026
x + d*a = [2,071140, 1,035590]
f(x + d*b) = 0,000026
x + d*b = [2,071140, 1,035590]
***************************************
Iteration 78
Gradient at x = (2,071140, 1,035590) is: (-0,001359, -0,000162).
Optimal stepsize is between: [0,830521, 0,830591]
f(x + d*a) = 0,000025
x + d*a = [2,070011, 1,035456]
f(x + d*b) = 0,000025
x + d*b = [2,070011, 1,035456]
***************************************
Iteration 79
Gradient at x = (2,070011, 1,035456) is: (0,000429, -0,003603).
Optimal stepsize is between: [0,112881, 0,112952]
f(x + d*a) = 0,000024
x + d*a = [2,070059, 1,035049]
f(x + d*b) = 0,000024
x + d*b = [2,070060, 1,035049]
***************************************
Iteration 80
Gradient at x = (2,070059, 1,035049) is: (-0,001298, -0,000156).
Optimal stepsize is between: [0,836124, 0,836195]
f(x + d*a) = 0,000023
x + d*a = [2,068974, 1,034919]
f(x + d*b) = 0,000023
x + d*b = [2,068974, 1,034919]
***************************************
Iteration 81
Gradient at x = (2,068974, 1,034919) is: (0,000414, -0,003454).
Optimal stepsize is between: [0,112794, 0,112865]
f(x + d*a) = 0,000023
x + d*a = [2,069021, 1,034529]
f(x + d*b) = 0,000023
x + d*b = [2,069021, 1,034529]
***************************************
Iteration 82
Gradient at x = (2,069021, 1,034529) is: (-0,001240, -0,000150).
Optimal stepsize is between: [0,842583, 0,842627]
f(x + d*a) = 0,000022
x + d*a = [2,067976, 1,034403]
f(x + d*b) = 0,000022
x + d*b = [2,067976, 1,034403]
***************************************
Iteration 83
Gradient at x = (2,067976, 1,034403) is: (0,000402, -0,003317).
Optimal stepsize is between: [0,112723, 0,112767]
f(x + d*a) = 0,000021
x + d*a = [2,068022, 1,034029]
f(x + d*b) = 0,000021
x + d*b = [2,068022, 1,034029]
***************************************
Iteration 84
Gradient at x = (2,068022, 1,034029) is: (-0,001187, -0,000145).
Optimal stepsize is between: [0,846275, 0,846346]
f(x + d*a) = 0,000021
x + d*a = [2,067017, 1,033907]
f(x + d*b) = 0,000021
x + d*b = [2,067017, 1,033907]
***************************************
Iteration 85
Gradient at x = (2,067017, 1,033907) is: (0,000388, -0,003183).
Optimal stepsize is between: [0,112680, 0,112750]
f(x + d*a) = 0,000020
x + d*a = [2,067061, 1,033548]
f(x + d*b) = 0,000020
x + d*b = [2,067061, 1,033548]
***************************************
Iteration 86
Gradient at x = (2,067061, 1,033548) is: (-0,001137, -0,000139).
Optimal stepsize is between: [0,849755, 0,849826]
f(x + d*a) = 0,000020
x + d*a = [2,066095, 1,033430]
f(x + d*b) = 0,000020
x + d*b = [2,066095, 1,033430]
***************************************
Iteration 87
Gradient at x = (2,066095, 1,033430) is: (0,000374, -0,003058).
Optimal stepsize is between: [0,112609, 0,112680]
f(x + d*a) = 0,000019
x + d*a = [2,066137, 1,033085]
f(x + d*b) = 0,000019
x + d*b = [2,066137, 1,033085]
***************************************
Iteration 88
Gradient at x = (2,066137, 1,033085) is: (-0,001090, -0,000135).
Optimal stepsize is between: [0,856513, 0,856557]
f(x + d*a) = 0,000019
x + d*a = [2,065204, 1,032970]
f(x + d*b) = 0,000019
x + d*b = [2,065204, 1,032970]
***************************************
Iteration 89
Gradient at x = (2,065204, 1,032970) is: (0,000364, -0,002946).
Optimal stepsize is between: [0,112538, 0,112582]
f(x + d*a) = 0,000018
x + d*a = [2,065245, 1,032639]
f(x + d*b) = 0,000018
x + d*b = [2,065245, 1,032638]
***************************************
Iteration 90
Gradient at x = (2,065245, 1,032639) is: (-0,001046, -0,000130).
Optimal stepsize is between: [0,859949, 0,859993]
f(x + d*a) = 0,000018
x + d*a = [2,064345, 1,032527]
f(x + d*b) = 0,000018
x + d*b = [2,064345, 1,032527]
***************************************
Iteration 91
Gradient at x = (2,064345, 1,032527) is: (0,000352, -0,002835).
Optimal stepsize is between: [0,112494, 0,112565]
f(x + d*a) = 0,000017
x + d*a = [2,064385, 1,032208]
f(x + d*b) = 0,000017
x + d*b = [2,064385, 1,032208]
***************************************
Iteration 92
Gradient at x = (2,064385, 1,032208) is: (-0,001005, -0,000125).
Optimal stepsize is between: [0,863598, 0,863642]
f(x + d*a) = 0,000017
x + d*a = [2,063517, 1,032100]
f(x + d*b) = 0,000017
x + d*b = [2,063517, 1,032100]
***************************************
Iteration 93
Gradient at x = (2,063517, 1,032100) is: (0,000341, -0,002731).
Optimal stepsize is between: [0,112424, 0,112494]
f(x + d*a) = 0,000016
x + d*a = [2,063555, 1,031793]
f(x + d*b) = 0,000016
x + d*b = [2,063555, 1,031793]
***************************************
Iteration 94
Gradient at x = (2,063555, 1,031793) is: (-0,000966, -0,000122).
Optimal stepsize is between: [0,870198, 0,870269]
f(x + d*a) = 0,000016
x + d*a = [2,062714, 1,031687]
f(x + d*b) = 0,000016
x + d*b = [2,062714, 1,031687]
***************************************
Iteration 95
Gradient at x = (2,062714, 1,031687) is: (0,000332, -0,002637).
Optimal stepsize is between: [0,112353, 0,112397]
f(x + d*a) = 0,000016
x + d*a = [2,062752, 1,031391]
f(x + d*b) = 0,000016
x + d*b = [2,062752, 1,031390]
***************************************
Iteration 96
Gradient at x = (2,062752, 1,031391) is: (-0,000930, -0,000118).
Optimal stepsize is between: [0,873934, 0,874004]
f(x + d*a) = 0,000015
x + d*a = [2,061939, 1,031288]
f(x + d*b) = 0,000015
x + d*b = [2,061939, 1,031288]
***************************************
Iteration 97
Gradient at x = (2,061939, 1,031288) is: (0,000322, -0,002545).
Optimal stepsize is between: [0,112282, 0,112353]
f(x + d*a) = 0,000015
x + d*a = [2,061976, 1,031002]
f(x + d*b) = 0,000015
x + d*b = [2,061976, 1,031002]
***************************************
Iteration 98
Gradient at x = (2,061976, 1,031002) is: (-0,000895, -0,000114).
Optimal stepsize is between: [0,880550, 0,880594]
f(x + d*a) = 0,000014
x + d*a = [2,061187, 1,030901]
f(x + d*b) = 0,000014
x + d*b = [2,061187, 1,030901]
***************************************
Iteration 99
Gradient at x = (2,061187, 1,030901) is: (0,000314, -0,002461).
Optimal stepsize is between: [0,112195, 0,112266]
f(x + d*a) = 0,000014
x + d*a = [2,061223, 1,030625]
f(x + d*b) = 0,000014
x + d*b = [2,061223, 1,030625]
***************************************
Iteration 100
Gradient at x = (2,061223, 1,030625) is: (-0,000862, -0,000111).
Optimal stepsize is between: [0,886366, 0,886437]
f(x + d*a) = 0,000014
x + d*a = [2,060458, 1,030527]
f(x + d*b) = 0,000014
x + d*b = [2,060458, 1,030527]
***************************************
Iteration 101
Gradient at x = (2,060458, 1,030527) is: (0,000307, -0,002381).
Optimal stepsize is between: [0,112124, 0,112195]
f(x + d*a) = 0,000013
x + d*a = [2,060493, 1,030260]
f(x + d*b) = 0,000013
x + d*b = [2,060493, 1,030260]
***************************************
Iteration 102
Gradient at x = (2,060493, 1,030260) is: (-0,000832, -0,000108).
Optimal stepsize is between: [0,891115, 0,891159]
f(x + d*a) = 0,000013
x + d*a = [2,059752, 1,030164]
f(x + d*b) = 0,000013
x + d*b = [2,059752, 1,030164]
***************************************
Iteration 103
Gradient at x = (2,059752, 1,030164) is: (0,000299, -0,002304).
Optimal stepsize is between: [0,112053, 0,112124]
f(x + d*a) = 0,000013
x + d*a = [2,059785, 1,029906]
f(x + d*b) = 0,000013
x + d*b = [2,059785, 1,029905]
***************************************
Iteration 104
Gradient at x = (2,059785, 1,029906) is: (-0,000802, -0,000105).
Optimal stepsize is between: [0,896887, 0,896958]
f(x + d*a) = 0,000012
x + d*a = [2,059065, 1,029812]
f(x + d*b) = 0,000012
x + d*b = [2,059065, 1,029812]
***************************************
Iteration 105
Gradient at x = (2,059065, 1,029812) is: (0,000291, -0,002231).
Optimal stepsize is between: [0,111983, 0,112026]
f(x + d*a) = 0,000012
x + d*a = [2,059098, 1,029562]
f(x + d*b) = 0,000012
x + d*b = [2,059098, 1,029562]
***************************************
Iteration 106
Gradient at x = (2,059098, 1,029562) is: (-0,000775, -0,000102).
Optimal stepsize is between: [0,901751, 0,901821]
f(x + d*a) = 0,000012
x + d*a = [2,058400, 1,029470]
f(x + d*b) = 0,000012
x + d*b = [2,058399, 1,029470]
***************************************
Iteration 107
Gradient at x = (2,058400, 1,029470) is: (0,000284, -0,002162).
Optimal stepsize is between: [0,111895, 0,111966]
f(x + d*a) = 0,000012
x + d*a = [2,058431, 1,029228]
f(x + d*b) = 0,000012
x + d*b = [2,058431, 1,029228]
***************************************
Iteration 108
Gradient at x = (2,058431, 1,029228) is: (-0,000748, -0,000099).
Optimal stepsize is between: [0,909805, 0,909875]
f(x + d*a) = 0,000011
x + d*a = [2,057751, 1,029138]
f(x + d*b) = 0,000011
x + d*b = [2,057750, 1,029138]
***************************************
Iteration 109
Gradient at x = (2,057751, 1,029138) is: (0,000279, -0,002099).
Optimal stepsize is between: [0,111798, 0,111868]
f(x + d*a) = 0,000011
x + d*a = [2,057782, 1,028903]
f(x + d*b) = 0,000011
x + d*b = [2,057782, 1,028903]
***************************************
Iteration 110
Gradient at x = (2,057782, 1,028903) is: (-0,000723, -0,000097).
Optimal stepsize is between: [0,916192, 0,916236]
f(x + d*a) = 0,000011
x + d*a = [2,057119, 1,028814]
f(x + d*b) = 0,000011
x + d*b = [2,057119, 1,028814]
***************************************
Iteration 111
Gradient at x = (2,057119, 1,028814) is: (0,000273, -0,002037).
Optimal stepsize is between: [0,111710, 0,111781]
f(x + d*a) = 0,000011
x + d*a = [2,057150, 1,028587]
f(x + d*b) = 0,000011
x + d*b = [2,057150, 1,028586]
***************************************
Iteration 112
Gradient at x = (2,057150, 1,028587) is: (-0,000699, -0,000095).
Optimal stepsize is between: [0,922978, 0,923048]
f(x + d*a) = 0,000010
x + d*a = [2,056504, 1,028499]
f(x + d*b) = 0,000010
x + d*b = [2,056504, 1,028499]
***************************************
Iteration 113
Gradient at x = (2,056504, 1,028499) is: (0,000267, -0,001978).
Optimal stepsize is between: [0,111640, 0,111710]
f(x + d*a) = 0,000010
x + d*a = [2,056534, 1,028279]
f(x + d*b) = 0,000010
x + d*b = [2,056534, 1,028278]
***************************************
Iteration 114
Gradient at x = (2,056534, 1,028279) is: (-0,000677, -0,000092).
Optimal stepsize is between: [0,927269, 0,927340]
f(x + d*a) = 0,000010
x + d*a = [2,055906, 1,028193]
f(x + d*b) = 0,000010
x + d*b = [2,055906, 1,028193]
***************************************
Iteration 115
Gradient at x = (2,055906, 1,028193) is: (0,000261, -0,001920).
Optimal stepsize is between: [0,111569, 0,111640]
f(x + d*a) = 0,000010
x + d*a = [2,055936, 1,027979]
f(x + d*b) = 0,000010
x + d*b = [2,055936, 1,027979]
***************************************
Iteration 116
Gradient at x = (2,055936, 1,027979) is: (-0,000655, -0,000090).
Optimal stepsize is between: [0,934141, 0,934212]
f(x + d*a) = 0,000010
x + d*a = [2,055324, 1,027895]
f(x + d*b) = 0,000010
x + d*b = [2,055324, 1,027895]
***************************************
Iteration 117
Gradient at x = (2,055324, 1,027895) is: (0,000256, -0,001866).
Optimal stepsize is between: [0,111498, 0,111542]
f(x + d*a) = 0,000009
x + d*a = [2,055352, 1,027687]
f(x + d*b) = 0,000009
x + d*b = [2,055352, 1,027687]
***************************************
Iteration 118
Gradient at x = (2,055352, 1,027687) is: (-0,000635, -0,000087).
Optimal stepsize is between: [0,938547, 0,938618]
f(x + d*a) = 0,000009
x + d*a = [2,054757, 1,027605]
f(x + d*b) = 0,000009
x + d*b = [2,054756, 1,027605]
***************************************
Iteration 119
Gradient at x = (2,054757, 1,027605) is: (0,000250, -0,001813).
Optimal stepsize is between: [0,111454, 0,111498]
f(x + d*a) = 0,000009
x + d*a = [2,054784, 1,027403]
f(x + d*b) = 0,000009
x + d*b = [2,054784, 1,027403]
***************************************
Iteration 120
Gradient at x = (2,054784, 1,027403) is: (-0,000615, -0,000085).
Optimal stepsize is between: [0,941640, 0,941711]
f(x + d*a) = 0,000009
x + d*a = [2,054205, 1,027323]
f(x + d*b) = 0,000009
x + d*b = [2,054205, 1,027323]
***************************************
Iteration 121
Gradient at x = (2,054205, 1,027323) is: (0,000244, -0,001761).
Optimal stepsize is between: [0,111411, 0,111481]
f(x + d*a) = 0,000009
x + d*a = [2,054232, 1,027126]
f(x + d*b) = 0,000009
x + d*b = [2,054232, 1,027126]
***************************************
Iteration 122
Gradient at x = (2,054232, 1,027126) is: (-0,000597, -0,000083).
Optimal stepsize is between: [0,946089, 0,946160]
f(x + d*a) = 0,000008
x + d*a = [2,053668, 1,027048]
f(x + d*b) = 0,000008
x + d*b = [2,053668, 1,027048]
***************************************
Iteration 123
Gradient at x = (2,053668, 1,027048) is: (0,000238, -0,001713).
Optimal stepsize is between: [0,111340, 0,111411]
f(x + d*a) = 0,000008
x + d*a = [2,053694, 1,026857]
f(x + d*b) = 0,000008
x + d*b = [2,053694, 1,026857]
***************************************
Iteration 124
Gradient at x = (2,053694, 1,026857) is: (-0,000579, -0,000081).
Optimal stepsize is between: [0,953103, 0,953174]
f(x + d*a) = 0,000008
x + d*a = [2,053143, 1,026780]
f(x + d*b) = 0,000008
x + d*b = [2,053143, 1,026780]
***************************************
Iteration 125
Gradient at x = (2,053143, 1,026780) is: (0,000234, -0,001669).
Optimal stepsize is between: [0,111269, 0,111313]
f(x + d*a) = 0,000008
x + d*a = [2,053169, 1,026594]
f(x + d*b) = 0,000008
x + d*b = [2,053169, 1,026594]
***************************************
Iteration 126
Gradient at x = (2,053169, 1,026594) is: (-0,000562, -0,000079).
Optimal stepsize is between: [0,957667, 0,957711]
f(x + d*a) = 0,000008
x + d*a = [2,052631, 1,026519]
f(x + d*b) = 0,000008
x + d*b = [2,052631, 1,026519]
***************************************
Iteration 127
Gradient at x = (2,052631, 1,026519) is: (0,000229, -0,001624).
Optimal stepsize is between: [0,111199, 0,111269]
f(x + d*a) = 0,000008
x + d*a = [2,052656, 1,026338]
f(x + d*b) = 0,000008
x + d*b = [2,052656, 1,026338]
***************************************
Iteration 128
Gradient at x = (2,052656, 1,026338) is: (-0,000545, -0,000078).
Optimal stepsize is between: [0,964795, 0,964866]
f(x + d*a) = 0,000008
x + d*a = [2,052130, 1,026263]
f(x + d*b) = 0,000008
x + d*b = [2,052130, 1,026263]
***************************************
Iteration 129
Gradient at x = (2,052130, 1,026263) is: (0,000225, -0,001583).
Optimal stepsize is between: [0,111128, 0,111172]
f(x + d*a) = 0,000007
x + d*a = [2,052155, 1,026087]
f(x + d*b) = 0,000007
x + d*b = [2,052155, 1,026087]
***************************************
Iteration 130
Gradient at x = (2,052155, 1,026087) is: (-0,000530, -0,000076).
Optimal stepsize is between: [0,969484, 0,969555]
f(x + d*a) = 0,000007
x + d*a = [2,051642, 1,026014]
f(x + d*b) = 0,000007
x + d*b = [2,051642, 1,026014]
***************************************
Iteration 131
Gradient at x = (2,051642, 1,026014) is: (0,000220, -0,001543).
Optimal stepsize is between: [0,111084, 0,111128]
f(x + d*a) = 0,000007
x + d*a = [2,051666, 1,025842]
f(x + d*b) = 0,000007
x + d*b = [2,051666, 1,025842]
***************************************
Iteration 132
Gradient at x = (2,051666, 1,025842) is: (-0,000515, -0,000074).
Optimal stepsize is between: [0,972550, 0,972621]
f(x + d*a) = 0,000007
x + d*a = [2,051166, 1,025771]
f(x + d*b) = 0,000007
x + d*b = [2,051166, 1,025771]
***************************************
Iteration 133
Gradient at x = (2,051166, 1,025771) is: (0,000215, -0,001502).
Optimal stepsize is between: [0,111040, 0,111111]
f(x + d*a) = 0,000007
x + d*a = [2,051190, 1,025604]
f(x + d*b) = 0,000007
x + d*b = [2,051190, 1,025604]
***************************************
Iteration 134
Gradient at x = (2,051190, 1,025604) is: (-0,000500, -0,000072).
Optimal stepsize is between: [0,977326, 0,977396]
f(x + d*a) = 0,000007
x + d*a = [2,050701, 1,025533]
f(x + d*b) = 0,000007
x + d*b = [2,050701, 1,025533]
***************************************
Iteration 135
Gradient at x = (2,050701, 1,025533) is: (0,000211, -0,001465).
Optimal stepsize is between: [0,110970, 0,111040]
f(x + d*a) = 0,000007
x + d*a = [2,050724, 1,025371]
f(x + d*b) = 0,000007
x + d*b = [2,050724, 1,025371]
***************************************
Iteration 136
Gradient at x = (2,050724, 1,025371) is: (-0,000487, -0,000071).
Optimal stepsize is between: [0,984639, 0,984710]
f(x + d*a) = 0,000007
x + d*a = [2,050245, 1,025301]
f(x + d*b) = 0,000007
x + d*b = [2,050245, 1,025301]
***************************************
Iteration 137
Gradient at x = (2,050245, 1,025301) is: (0,000208, -0,001430).
Optimal stepsize is between: [0,110899, 0,110943]
f(x + d*a) = 0,000006
x + d*a = [2,050268, 1,025143]
f(x + d*b) = 0,000006
x + d*b = [2,050268, 1,025142]
***************************************
Iteration 138
Gradient at x = (2,050268, 1,025143) is: (-0,000473, -0,000069).
Optimal stepsize is between: [0,989503, 0,989573]
f(x + d*a) = 0,000006
x + d*a = [2,049799, 1,025074]
f(x + d*b) = 0,000006
x + d*b = [2,049799, 1,025074]
***************************************
Iteration 139
Gradient at x = (2,049799, 1,025074) is: (0,000204, -0,001396).
Optimal stepsize is between: [0,110828, 0,110899]
f(x + d*a) = 0,000006
x + d*a = [2,049822, 1,024919]
f(x + d*b) = 0,000006
x + d*b = [2,049822, 1,024919]
***************************************
Iteration 140
Gradient at x = (2,049822, 1,024919) is: (-0,000461, -0,000068).
Optimal stepsize is between: [0,997001, 0,997045]
f(x + d*a) = 0,000006
x + d*a = [2,049363, 1,024852]
f(x + d*b) = 0,000006
x + d*b = [2,049363, 1,024852]
***************************************
Iteration 141
Gradient at x = (2,049363, 1,024852) is: (0,000201, -0,001364).
Optimal stepsize is between: [0,110741, 0,110812]
f(x + d*a) = 0,000006
x + d*a = [2,049385, 1,024701]
f(x + d*b) = 0,000006
x + d*b = [2,049385, 1,024701]
***************************************
Iteration 142
Gradient at x = (2,049385, 1,024701) is: (-0,000448, -0,000067).
Optimal stepsize is between: [1,004658, 1,004729]
f(x + d*a) = 0,000006
x + d*a = [2,048934, 1,024634]
f(x + d*b) = 0,000006
x + d*b = [2,048934, 1,024634]
***************************************
Iteration 143
Gradient at x = (2,048934, 1,024634) is: (0,000198, -0,001333).
Optimal stepsize is between: [0,110670, 0,110714]
f(x + d*a) = 0,000006
x + d*a = [2,048956, 1,024486]
f(x + d*b) = 0,000006
x + d*b = [2,048956, 1,024486]
***************************************
Iteration 144
Gradient at x = (2,048956, 1,024486) is: (-0,000437, -0,000065).
Optimal stepsize is between: [1,009646, 1,009717]
f(x + d*a) = 0,000006
x + d*a = [2,048515, 1,024420]
f(x + d*b) = 0,000006
x + d*b = [2,048515, 1,024420]
***************************************
Iteration 145
Gradient at x = (2,048515, 1,024420) is: (0,000194, -0,001302).
Optimal stepsize is between: [0,110599, 0,110670]
f(x + d*a) = 0,000006
x + d*a = [2,048537, 1,024276]
f(x + d*b) = 0,000006
x + d*b = [2,048537, 1,024276]
***************************************
Iteration 146
Gradient at x = (2,048537, 1,024276) is: (-0,000425, -0,000064).
Optimal stepsize is between: [1,017488, 1,017558]
f(x + d*a) = 0,000005
x + d*a = [2,048104, 1,024211]
f(x + d*b) = 0,000005
x + d*b = [2,048104, 1,024211]
***************************************
Iteration 147
Gradient at x = (2,048104, 1,024211) is: (0,000192, -0,001274).
Optimal stepsize is between: [0,110529, 0,110572]
f(x + d*a) = 0,000005
x + d*a = [2,048125, 1,024070]
f(x + d*b) = 0,000005
x + d*b = [2,048125, 1,024070]
***************************************
Iteration 148
Gradient at x = (2,048125, 1,024070) is: (-0,000414, -0,000063).
Optimal stepsize is between: [1,022563, 1,022634]
f(x + d*a) = 0,000005
x + d*a = [2,047701, 1,024006]
f(x + d*b) = 0,000005
x + d*b = [2,047701, 1,024006]
***************************************
Iteration 149
Gradient at x = (2,047701, 1,024006) is: (0,000188, -0,001245).
Optimal stepsize is between: [0,110485, 0,110529]
f(x + d*a) = 0,000005
x + d*a = [2,047722, 1,023869]
f(x + d*b) = 0,000005
x + d*b = [2,047722, 1,023869]
***************************************
Iteration 150
Gradient at x = (2,047722, 1,023869) is: (-0,000404, -0,000061).
Optimal stepsize is between: [1,025972, 1,026016]
f(x + d*a) = 0,000005
x + d*a = [2,047308, 1,023806]
f(x + d*b) = 0,000005
x + d*b = [2,047307, 1,023806]
***************************************
Iteration 151
Gradient at x = (2,047308, 1,023806) is: (0,000185, -0,001216).
Optimal stepsize is between: [0,110441, 0,110512]
f(x + d*a) = 0,000005
x + d*a = [2,047328, 1,023671]
f(x + d*b) = 0,000005
x + d*b = [2,047328, 1,023671]
***************************************
Iteration 152
Gradient at x = (2,047328, 1,023671) is: (-0,000394, -0,000060).
Optimal stepsize is between: [1,031004, 1,031048]
f(x + d*a) = 0,000005
x + d*a = [2,046922, 1,023609]
f(x + d*b) = 0,000005
x + d*b = [2,046922, 1,023609]
***************************************
Iteration 153
Gradient at x = (2,046922, 1,023609) is: (0,000181, -0,001189).
Optimal stepsize is between: [0,110371, 0,110441]
f(x + d*a) = 0,000005
x + d*a = [2,046942, 1,023478]
f(x + d*b) = 0,000005
x + d*b = [2,046942, 1,023478]
***************************************
Iteration 154
Gradient at x = (2,046942, 1,023478) is: (-0,000384, -0,000059).
Optimal stepsize is between: [1,039287, 1,039358]
f(x + d*a) = 0,000005
x + d*a = [2,046542, 1,023417]
f(x + d*b) = 0,000005
x + d*b = [2,046542, 1,023417]
***************************************
Iteration 155
Gradient at x = (2,046542, 1,023417) is: (0,000179, -0,001165).
Optimal stepsize is between: [0,110300, 0,110344]
f(x + d*a) = 0,000005
x + d*a = [2,046562, 1,023288]
f(x + d*b) = 0,000005
x + d*b = [2,046562, 1,023288]
***************************************
Iteration 156
Gradient at x = (2,046562, 1,023288) is: (-0,000375, -0,000058).
Optimal stepsize is between: [1,044433, 1,044504]
f(x + d*a) = 0,000005
x + d*a = [2,046171, 1,023228]
f(x + d*b) = 0,000005
x + d*b = [2,046171, 1,023228]
***************************************
Iteration 157
Gradient at x = (2,046171, 1,023228) is: (0,000176, -0,001140).
Optimal stepsize is between: [0,110229, 0,110300]
f(x + d*a) = 0,000005
x + d*a = [2,046190, 1,023102]
f(x + d*b) = 0,000005
x + d*b = [2,046190, 1,023102]
***************************************
Iteration 158
Gradient at x = (2,046190, 1,023102) is: (-0,000366, -0,000057).
Optimal stepsize is between: [1,052918, 1,052988]
f(x + d*a) = 0,000004
x + d*a = [2,045805, 1,023042]
f(x + d*b) = 0,000004
x + d*b = [2,045805, 1,023042]
***************************************
Iteration 159
Gradient at x = (2,045805, 1,023042) is: (0,000174, -0,001117).
Optimal stepsize is between: [0,110142, 0,110213]
f(x + d*a) = 0,000004
x + d*a = [2,045824, 1,022919]
f(x + d*b) = 0,000004
x + d*b = [2,045824, 1,022919]
***************************************
Iteration 160
Gradient at x = (2,045824, 1,022919) is: (-0,000357, -0,000056).
Optimal stepsize is between: [1,061315, 1,061359]
f(x + d*a) = 0,000004
x + d*a = [2,045446, 1,022860]
f(x + d*b) = 0,000004
x + d*b = [2,045445, 1,022860]
***************************************
Iteration 161
Gradient at x = (2,045446, 1,022860) is: (0,000172, -0,001095).
Optimal stepsize is between: [0,110071, 0,110142]
f(x + d*a) = 0,000004
x + d*a = [2,045464, 1,022739]
f(x + d*b) = 0,000004
x + d*b = [2,045464, 1,022739]
***************************************
Iteration 162
Gradient at x = (2,045464, 1,022739) is: (-0,000348, -0,000055).
Optimal stepsize is between: [1,066875, 1,066919]
f(x + d*a) = 0,000004
x + d*a = [2,045093, 1,022680]
f(x + d*b) = 0,000004
x + d*b = [2,045093, 1,022680]
***************************************
Iteration 163
Gradient at x = (2,045093, 1,022680) is: (0,000169, -0,001072).
Optimal stepsize is between: [0,110000, 0,110071]
f(x + d*a) = 0,000004
x + d*a = [2,045111, 1,022562]
f(x + d*b) = 0,000004
x + d*b = [2,045111, 1,022562]
***************************************
Iteration 164
Gradient at x = (2,045111, 1,022562) is: (-0,000340, -0,000054).
Optimal stepsize is between: [1,075528, 1,075599]
f(x + d*a) = 0,000004
x + d*a = [2,044746, 1,022504]
f(x + d*b) = 0,000004
x + d*b = [2,044746, 1,022504]
***************************************
Iteration 165
Gradient at x = (2,044746, 1,022504) is: (0,000167, -0,001052).
Optimal stepsize is between: [0,109930, 0,109973]
f(x + d*a) = 0,000004
x + d*a = [2,044764, 1,022389]
f(x + d*b) = 0,000004
x + d*b = [2,044764, 1,022389]
***************************************
Iteration 166
Gradient at x = (2,044764, 1,022389) is: (-0,000332, -0,000053).
Optimal stepsize is between: [1,081246, 1,081290]
f(x + d*a) = 0,000004
x + d*a = [2,044405, 1,022331]
f(x + d*b) = 0,000004
x + d*b = [2,044405, 1,022331]
***************************************
Iteration 167
Gradient at x = (2,044405, 1,022331) is: (0,000165, -0,001030).
Optimal stepsize is between: [0,109886, 0,109930]
f(x + d*a) = 0,000004
x + d*a = [2,044423, 1,022218]
f(x + d*b) = 0,000004
x + d*b = [2,044423, 1,022218]
***************************************
Iteration 168
Gradient at x = (2,044423, 1,022218) is: (-0,000325, -0,000052).
Optimal stepsize is between: [1,084797, 1,084868]
f(x + d*a) = 0,000004
x + d*a = [2,044071, 1,022161]
f(x + d*b) = 0,000004
x + d*b = [2,044071, 1,022161]
***************************************
Iteration 169
Gradient at x = (2,044071, 1,022161) is: (0,000162, -0,001008).
Optimal stepsize is between: [0,109842, 0,109913]
f(x + d*a) = 0,000004
x + d*a = [2,044089, 1,022051]
f(x + d*b) = 0,000004
x + d*b = [2,044089, 1,022051]
***************************************
Iteration 170
Gradient at x = (2,044089, 1,022051) is: (-0,000317, -0,000051).
Optimal stepsize is between: [1,090656, 1,090700]
f(x + d*a) = 0,000004
x + d*a = [2,043743, 1,021995]
f(x + d*b) = 0,000004
x + d*b = [2,043743, 1,021995]
***************************************
Iteration 171
Gradient at x = (2,043743, 1,021995) is: (0,000159, -0,000989).
Optimal stepsize is between: [0,109772, 0,109842]
f(x + d*a) = 0,000004
x + d*a = [2,043760, 1,021886]
f(x + d*b) = 0,000004
x + d*b = [2,043760, 1,021886]
***************************************
Iteration 172
Gradient at x = (2,043760, 1,021886) is: (-0,000310, -0,000050).
Optimal stepsize is between: [1,099565, 1,099636]
f(x + d*a) = 0,000004
x + d*a = [2,043419, 1,021831]
f(x + d*b) = 0,000004
x + d*b = [2,043419, 1,021831]
***************************************
Iteration 173
Gradient at x = (2,043419, 1,021831) is: (0,000158, -0,000970).
Optimal stepsize is between: [0,109701, 0,109745]
f(x + d*a) = 0,000004
x + d*a = [2,043437, 1,021724]
f(x + d*b) = 0,000004
x + d*b = [2,043437, 1,021724]
***************************************
Iteration 174
Gradient at x = (2,043437, 1,021724) is: (-0,000303, -0,000050).
Optimal stepsize is between: [1,105697, 1,105741]
f(x + d*a) = 0,000004
x + d*a = [2,043101, 1,021670]
f(x + d*b) = 0,000004
x + d*b = [2,043101, 1,021670]
***************************************
Iteration 175
Gradient at x = (2,043101, 1,021670) is: (0,000156, -0,000952).
Optimal stepsize is between: [0,109630, 0,109701]
f(x + d*a) = 0,000003
x + d*a = [2,043118, 1,021565]
f(x + d*b) = 0,000003
x + d*b = [2,043118, 1,021565]
***************************************
Iteration 176
Gradient at x = (2,043118, 1,021565) is: (-0,000296, -0,000049).
Optimal stepsize is between: [1,114721, 1,114791]
f(x + d*a) = 0,000003
x + d*a = [2,042788, 1,021511]
f(x + d*b) = 0,000003
x + d*b = [2,042788, 1,021511]
***************************************
Iteration 177
Gradient at x = (2,042788, 1,021511) is: (0,000154, -0,000935).
Optimal stepsize is between: [0,109559, 0,109603]
f(x + d*a) = 0,000003
x + d*a = [2,042805, 1,021409]
f(x + d*b) = 0,000003
x + d*b = [2,042805, 1,021409]
***************************************
Iteration 178
Gradient at x = (2,042805, 1,021409) is: (-0,000290, -0,000048).
Optimal stepsize is between: [1,121152, 1,121196]
f(x + d*a) = 0,000003
x + d*a = [2,042480, 1,021355]
f(x + d*b) = 0,000003
x + d*b = [2,042480, 1,021355]
***************************************
Iteration 179
Gradient at x = (2,042480, 1,021355) is: (0,000152, -0,000917).
Optimal stepsize is between: [0,109516, 0,109559]
f(x + d*a) = 0,000003
x + d*a = [2,042497, 1,021254]
f(x + d*b) = 0,000003
x + d*b = [2,042497, 1,021254]
***************************************
Iteration 180
Gradient at x = (2,042497, 1,021254) is: (-0,000283, -0,000047).
Optimal stepsize is between: [1,124616, 1,124686]
f(x + d*a) = 0,000003
x + d*a = [2,042178, 1,021201]
f(x + d*b) = 0,000003
x + d*b = [2,042178, 1,021201]
***************************************
Iteration 181
Gradient at x = (2,042178, 1,021201) is: (0,000149, -0,000899).
Optimal stepsize is between: [0,109472, 0,109543]
f(x + d*a) = 0,000003
x + d*a = [2,042194, 1,021103]
f(x + d*b) = 0,000003
x + d*b = [2,042194, 1,021103]
***************************************
Iteration 182
Gradient at x = (2,042194, 1,021103) is: (-0,000277, -0,000046).
Optimal stepsize is between: [1,131189, 1,131232]
f(x + d*a) = 0,000003
x + d*a = [2,041881, 1,021051]
f(x + d*b) = 0,000003
x + d*b = [2,041881, 1,021051]
***************************************
Iteration 183
Gradient at x = (2,041881, 1,021051) is: (0,000147, -0,000882).
Optimal stepsize is between: [0,109401, 0,109472]
f(x + d*a) = 0,000003
x + d*a = [2,041897, 1,020954]
f(x + d*b) = 0,000003
x + d*b = [2,041897, 1,020954]
***************************************
Iteration 184
Gradient at x = (2,041897, 1,020954) is: (-0,000271, -0,000046).
Optimal stepsize is between: [1,140512, 1,140582]
f(x + d*a) = 0,000003
x + d*a = [2,041587, 1,020902]
f(x + d*b) = 0,000003
x + d*b = [2,041587, 1,020902]
***************************************
Iteration 185
Gradient at x = (2,041587, 1,020902) is: (0,000146, -0,000867).
Optimal stepsize is between: [0,109331, 0,109374]
f(x + d*a) = 0,000003
x + d*a = [2,041603, 1,020807]
f(x + d*b) = 0,000003
x + d*b = [2,041603, 1,020807]
***************************************
Iteration 186
Gradient at x = (2,041603, 1,020807) is: (-0,000266, -0,000045).
Optimal stepsize is between: [1,147313, 1,147357]
f(x + d*a) = 0,000003
x + d*a = [2,041299, 1,020756]
f(x + d*b) = 0,000003
x + d*b = [2,041299, 1,020756]
***************************************
Iteration 187
Gradient at x = (2,041299, 1,020756) is: (0,000144, -0,000851).
Optimal stepsize is between: [0,109260, 0,109331]
f(x + d*a) = 0,000003
x + d*a = [2,041314, 1,020663]
f(x + d*b) = 0,000003
x + d*b = [2,041314, 1,020663]
***************************************
Iteration 188
Gradient at x = (2,041314, 1,020663) is: (-0,000260, -0,000044).
Optimal stepsize is between: [1,156865, 1,156936]
f(x + d*a) = 0,000003
x + d*a = [2,041014, 1,020611]
f(x + d*b) = 0,000003
x + d*b = [2,041014, 1,020611]
***************************************
Iteration 189
Gradient at x = (2,041014, 1,020611) is: (0,000143, -0,000837).
Optimal stepsize is between: [0,109173, 0,109243]
f(x + d*a) = 0,000003
x + d*a = [2,041029, 1,020520]
f(x + d*b) = 0,000003
x + d*b = [2,041029, 1,020520]
***************************************
Iteration 190
Gradient at x = (2,041029, 1,020520) is: (-0,000254, -0,000044).
Optimal stepsize is between: [1,167729, 1,167773]
f(x + d*a) = 0,000003
x + d*a = [2,040732, 1,020469]
f(x + d*b) = 0,000003
x + d*b = [2,040732, 1,020469]
***************************************
Iteration 191
Gradient at x = (2,040732, 1,020469) is: (0,000142, -0,000824).
Optimal stepsize is between: [0,109102, 0,109173]
f(x + d*a) = 0,000003
x + d*a = [2,040748, 1,020379]
f(x + d*b) = 0,000003
x + d*b = [2,040748, 1,020379]
***************************************
Iteration 192
Gradient at x = (2,040748, 1,020379) is: (-0,000249, -0,000043).
Optimal stepsize is between: [1,173730, 1,173801]
f(x + d*a) = 0,000003
x + d*a = [2,040455, 1,020329]
f(x + d*b) = 0,000003
x + d*b = [2,040455, 1,020329]
***************************************
Iteration 193
Gradient at x = (2,040455, 1,020329) is: (0,000140, -0,000809).
Optimal stepsize is between: [0,109031, 0,109102]
f(x + d*a) = 0,000003
x + d*a = [2,040470, 1,020240]
f(x + d*b) = 0,000003
x + d*b = [2,040470, 1,020240]
***************************************
Iteration 194
Gradient at x = (2,040470, 1,020240) is: (-0,000244, -0,000042).
Optimal stepsize is between: [1,184894, 1,184965]
f(x + d*a) = 0,000003
x + d*a = [2,040181, 1,020190]
f(x + d*b) = 0,000003
x + d*b = [2,040181, 1,020190]
***************************************
Iteration 195
Gradient at x = (2,040181, 1,020190) is: (0,000139, -0,000796).
Optimal stepsize is between: [0,108960, 0,109004]
f(x + d*a) = 0,000003
x + d*a = [2,040196, 1,020103]
f(x + d*b) = 0,000003
x + d*b = [2,040196, 1,020103]
***************************************
Iteration 196
Gradient at x = (2,040196, 1,020103) is: (-0,000239, -0,000042).
Optimal stepsize is between: [1,191168, 1,191211]
f(x + d*a) = 0,000003
x + d*a = [2,039912, 1,020054]
f(x + d*b) = 0,000003
x + d*b = [2,039912, 1,020054]
***************************************
Iteration 197
Gradient at x = (2,039912, 1,020054) is: (0,000137, -0,000782).
Optimal stepsize is between: [0,108917, 0,108960]
f(x + d*a) = 0,000003
x + d*a = [2,039927, 1,019969]
f(x + d*b) = 0,000003
x + d*b = [2,039927, 1,019968]
***************************************
Iteration 198
Gradient at x = (2,039927, 1,019969) is: (-0,000234, -0,000041).
Optimal stepsize is between: [1,196058, 1,196101]
f(x + d*a) = 0,000003
x + d*a = [2,039647, 1,019919]
f(x + d*b) = 0,000003
x + d*b = [2,039647, 1,019919]
***************************************
Iteration 199
Gradient at x = (2,039647, 1,019919) is: (0,000135, -0,000768).
Optimal stepsize is between: [0,108873, 0,108944]
f(x + d*a) = 0,000002
x + d*a = [2,039661, 1,019836]
f(x + d*b) = 0,000002
x + d*b = [2,039661, 1,019836]
***************************************
Iteration 200
Gradient at x = (2,039661, 1,019836) is: (-0,000229, -0,000040).
Optimal stepsize is between: [1,202446, 1,202489]
f(x + d*a) = 0,000002
x + d*a = [2,039386, 1,019787]
f(x + d*b) = 0,000002
x + d*b = [2,039386, 1,019787]
***************************************
Iteration 201
Gradient at x = (2,039386, 1,019787) is: (0,000133, -0,000755).
Optimal stepsize is between: [0,108802, 0,108873]
f(x + d*a) = 0,000002
x + d*a = [2,039400, 1,019705]
f(x + d*b) = 0,000002
x + d*b = [2,039400, 1,019705]
***************************************
Iteration 202
Gradient at x = (2,039400, 1,019705) is: (-0,000225, -0,000040).
Optimal stepsize is between: [1,214077, 1,214148]
f(x + d*a) = 0,000002
x + d*a = [2,039127, 1,019657]
f(x + d*b) = 0,000002
x + d*b = [2,039127, 1,019657]
***************************************
Iteration 203
Gradient at x = (2,039127, 1,019657) is: (0,000132, -0,000743).
Optimal stepsize is between: [0,108732, 0,108775]
f(x + d*a) = 0,000002
x + d*a = [2,039142, 1,019576]
f(x + d*b) = 0,000002
x + d*b = [2,039142, 1,019576]
***************************************
Iteration 204
Gradient at x = (2,039142, 1,019576) is: (-0,000220, -0,000039).
Optimal stepsize is between: [1,220721, 1,220792]
f(x + d*a) = 0,000002
x + d*a = [2,038873, 1,019528]
f(x + d*b) = 0,000002
x + d*b = [2,038873, 1,019528]
***************************************
Iteration 205
Gradient at x = (2,038873, 1,019528) is: (0,000130, -0,000731).
Optimal stepsize is between: [0,108661, 0,108732]
f(x + d*a) = 0,000002
x + d*a = [2,038887, 1,019448]
f(x + d*b) = 0,000002
x + d*b = [2,038887, 1,019448]
***************************************
Iteration 206
Gradient at x = (2,038887, 1,019448) is: (-0,000216, -0,000039).
Optimal stepsize is between: [1,232740, 1,232811]
f(x + d*a) = 0,000002
x + d*a = [2,038621, 1,019401]
f(x + d*b) = 0,000002
x + d*b = [2,038621, 1,019401]
***************************************
Iteration 207
Gradient at x = (2,038621, 1,019401) is: (0,000130, -0,000720).
Optimal stepsize is between: [0,108573, 0,108644]
f(x + d*a) = 0,000002
x + d*a = [2,038635, 1,019322]
f(x + d*b) = 0,000002
x + d*b = [2,038635, 1,019322]
***************************************
Iteration 208
Gradient at x = (2,038635, 1,019322) is: (-0,000212, -0,000038).
Optimal stepsize is between: [1,243877, 1,243947]
f(x + d*a) = 0,000002
x + d*a = [2,038372, 1,019275]
f(x + d*b) = 0,000002
x + d*b = [2,038372, 1,019275]
***************************************
Iteration 209
Gradient at x = (2,038372, 1,019275) is: (0,000129, -0,000709).
Optimal stepsize is between: [0,108503, 0,108573]
f(x + d*a) = 0,000002
x + d*a = [2,038386, 1,019198]
f(x + d*b) = 0,000002
x + d*b = [2,038386, 1,019198]
***************************************
Iteration 210
Gradient at x = (2,038386, 1,019198) is: (-0,000207, -0,000038).
Optimal stepsize is between: [1,252045, 1,252089]
f(x + d*a) = 0,000002
x + d*a = [2,038126, 1,019150]
f(x + d*b) = 0,000002
x + d*b = [2,038126, 1,019150]
***************************************
Iteration 211
Gradient at x = (2,038126, 1,019150) is: (0,000127, -0,000698).
Optimal stepsize is between: [0,108432, 0,108503]
f(x + d*a) = 0,000002
x + d*a = [2,038140, 1,019075]
f(x + d*b) = 0,000002
x + d*b = [2,038140, 1,019075]
***************************************
Iteration 212
Gradient at x = (2,038140, 1,019075) is: (-0,000203, -0,000037).
Optimal stepsize is between: [1,263421, 1,263492]
f(x + d*a) = 0,000002
x + d*a = [2,037883, 1,019028]
f(x + d*b) = 0,000002
x + d*b = [2,037883, 1,019028]
***************************************
Iteration 213
Gradient at x = (2,037883, 1,019028) is: (0,000126, -0,000687).
Optimal stepsize is between: [0,108361, 0,108405]
f(x + d*a) = 0,000002
x + d*a = [2,037897, 1,018953]
f(x + d*b) = 0,000002
x + d*b = [2,037897, 1,018953]
***************************************
Iteration 214
Gradient at x = (2,037897, 1,018953) is: (-0,000199, -0,000037).
Optimal stepsize is between: [1,271933, 1,272003]
f(x + d*a) = 0,000002
x + d*a = [2,037644, 1,018906]
f(x + d*b) = 0,000002
x + d*b = [2,037644, 1,018906]
***************************************
Iteration 215
Gradient at x = (2,037644, 1,018906) is: (0,000125, -0,000676).
Optimal stepsize is between: [0,108291, 0,108361]
f(x + d*a) = 0,000002
x + d*a = [2,037657, 1,018833]
f(x + d*b) = 0,000002
x + d*b = [2,037657, 1,018833]
***************************************
Iteration 216
Gradient at x = (2,037657, 1,018833) is: (-0,000195, -0,000036).
Optimal stepsize is between: [1,283695, 1,283739]
f(x + d*a) = 0,000002
x + d*a = [2,037406, 1,018786]
f(x + d*b) = 0,000002
x + d*b = [2,037406, 1,018786]
***************************************
Iteration 217
Gradient at x = (2,037406, 1,018786) is: (0,000124, -0,000667).
Optimal stepsize is between: [0,108203, 0,108274]
f(x + d*a) = 0,000002
x + d*a = [2,037420, 1,018714]
f(x + d*b) = 0,000002
x + d*b = [2,037420, 1,018714]
***************************************
Iteration 218
Gradient at x = (2,037420, 1,018714) is: (-0,000192, -0,000036).
Optimal stepsize is between: [1,297212, 1,297255]
f(x + d*a) = 0,000002
x + d*a = [2,037171, 1,018668]
f(x + d*b) = 0,000002
x + d*b = [2,037171, 1,018668]
***************************************
Iteration 219
Gradient at x = (2,037171, 1,018668) is: (0,000123, -0,000657).
Optimal stepsize is between: [0,108133, 0,108176]
f(x + d*a) = 0,000002
x + d*a = [2,037184, 1,018597]
f(x + d*b) = 0,000002
x + d*b = [2,037184, 1,018597]
***************************************
Iteration 220
Gradient at x = (2,037184, 1,018597) is: (-0,000188, -0,000035).
Optimal stepsize is between: [1,304667, 1,304737]
f(x + d*a) = 0,000002
x + d*a = [2,036939, 1,018550]
f(x + d*b) = 0,000002
x + d*b = [2,036939, 1,018550]
***************************************
Iteration 221
Gradient at x = (2,036939, 1,018550) is: (0,000122, -0,000647).
Optimal stepsize is between: [0,108062, 0,108133]
f(x + d*a) = 0,000002
x + d*a = [2,036952, 1,018481]
f(x + d*b) = 0,000002
x + d*b = [2,036952, 1,018480]
***************************************
Iteration 222
Gradient at x = (2,036952, 1,018481) is: (-0,000184, -0,000035).
Optimal stepsize is between: [1,318597, 1,318668]
f(x + d*a) = 0,000002
x + d*a = [2,036709, 1,018434]
f(x + d*b) = 0,000002
x + d*b = [2,036709, 1,018434]
***************************************
Iteration 223
Gradient at x = (2,036709, 1,018434) is: (0,000121, -0,000638).
Optimal stepsize is between: [0,107991, 0,108035]
f(x + d*a) = 0,000002
x + d*a = [2,036722, 1,018365]
f(x + d*b) = 0,000002
x + d*b = [2,036722, 1,018365]
***************************************
Iteration 224
Gradient at x = (2,036722, 1,018365) is: (-0,000181, -0,000034).
Optimal stepsize is between: [1,326395, 1,326439]
f(x + d*a) = 0,000002
x + d*a = [2,036482, 1,018320]
f(x + d*b) = 0,000002
x + d*b = [2,036482, 1,018320]
***************************************
Iteration 225
Gradient at x = (2,036482, 1,018320) is: (0,000120, -0,000628).
Optimal stepsize is between: [0,107947, 0,107991]
f(x + d*a) = 0,000002
x + d*a = [2,036495, 1,018252]
f(x + d*b) = 0,000002
x + d*b = [2,036495, 1,018252]
***************************************
Iteration 226
Gradient at x = (2,036495, 1,018252) is: (-0,000177, -0,000034).
Optimal stepsize is between: [1,332440, 1,332510]
f(x + d*a) = 0,000002
x + d*a = [2,036259, 1,018207]
f(x + d*b) = 0,000002
x + d*b = [2,036259, 1,018207]
***************************************
Iteration 227
Gradient at x = (2,036259, 1,018207) is: (0,000118, -0,000618).
Optimal stepsize is between: [0,107904, 0,107974]
f(x + d*a) = 0,000002
x + d*a = [2,036272, 1,018140]
f(x + d*b) = 0,000002
x + d*b = [2,036272, 1,018140]
***************************************
Iteration 228
Gradient at x = (2,036272, 1,018140) is: (-0,000174, -0,000033).
Optimal stepsize is between: [1,340423, 1,340494]
f(x + d*a) = 0,000002
x + d*a = [2,036038, 1,018095]
f(x + d*b) = 0,000002
x + d*b = [2,036038, 1,018095]
***************************************
Iteration 229
Gradient at x = (2,036038, 1,018095) is: (0,000117, -0,000608).
Optimal stepsize is between: [0,107833, 0,107904]
f(x + d*a) = 0,000002
x + d*a = [2,036051, 1,018030]
f(x + d*b) = 0,000002
x + d*b = [2,036051, 1,018030]
***************************************
Iteration 230
Gradient at x = (2,036051, 1,018030) is: (-0,000171, -0,000033).
Optimal stepsize is between: [1,355094, 1,355164]
f(x + d*a) = 0,000002
x + d*a = [2,035819, 1,017985]
f(x + d*b) = 0,000002
x + d*b = [2,035819, 1,017985]
***************************************
Iteration 231
Gradient at x = (2,035819, 1,017985) is: (0,000116, -0,000600).
Optimal stepsize is between: [0,107762, 0,107806]
f(x + d*a) = 0,000002
x + d*a = [2,035832, 1,017920]
f(x + d*b) = 0,000002
x + d*b = [2,035832, 1,017920]
***************************************
Iteration 232
Gradient at x = (2,035832, 1,017920) is: (-0,000168, -0,000033).
Optimal stepsize is between: [1,363350, 1,363393]
f(x + d*a) = 0,000002
x + d*a = [2,035603, 1,017876]
f(x + d*b) = 0,000002
x + d*b = [2,035603, 1,017876]
***************************************
Iteration 233
Gradient at x = (2,035603, 1,017876) is: (0,000115, -0,000591).
Optimal stepsize is between: [0,107692, 0,107762]
f(x + d*a) = 0,000002
x + d*a = [2,035616, 1,017812]
f(x + d*b) = 0,000002
x + d*b = [2,035616, 1,017812]
***************************************
Iteration 234
Gradient at x = (2,035616, 1,017812) is: (-0,000165, -0,000032).
Optimal stepsize is between: [1,378532, 1,378603]
f(x + d*a) = 0,000002
x + d*a = [2,035389, 1,017767]
f(x + d*b) = 0,000002
x + d*b = [2,035389, 1,017767]
***************************************
Iteration 235
Gradient at x = (2,035389, 1,017767) is: (0,000115, -0,000584).
Optimal stepsize is between: [0,107604, 0,107675]
f(x + d*a) = 0,000002
x + d*a = [2,035401, 1,017705]
f(x + d*b) = 0,000002
x + d*b = [2,035401, 1,017704]
***************************************
Iteration 236
Gradient at x = (2,035401, 1,017705) is: (-0,000162, -0,000032).
Optimal stepsize is between: [1,392691, 1,392735]
f(x + d*a) = 0,000002
x + d*a = [2,035176, 1,017660]
f(x + d*b) = 0,000002
x + d*b = [2,035176, 1,017660]
***************************************
Iteration 237
Gradient at x = (2,035176, 1,017660) is: (0,000114, -0,000576).
Optimal stepsize is between: [0,107533, 0,107604]
f(x + d*a) = 0,000002
x + d*a = [2,035188, 1,017598]
f(x + d*b) = 0,000002
x + d*b = [2,035188, 1,017598]
***************************************
Iteration 238
Gradient at x = (2,035188, 1,017598) is: (-0,000159, -0,000032).
Optimal stepsize is between: [1,402869, 1,402940]
f(x + d*a) = 0,000002
x + d*a = [2,034966, 1,017554]
f(x + d*b) = 0,000002
x + d*b = [2,034966, 1,017554]
***************************************
Iteration 239
Gradient at x = (2,034966, 1,017554) is: (0,000113, -0,000568).
Optimal stepsize is between: [0,107463, 0,107533]
f(x + d*a) = 0,000001
x + d*a = [2,034978, 1,017493]
f(x + d*b) = 0,000001
x + d*b = [2,034978, 1,017493]
***************************************
Iteration 240
Gradient at x = (2,034978, 1,017493) is: (-0,000156, -0,000031).
Optimal stepsize is between: [1,417540, 1,417610]
f(x + d*a) = 0,000001
x + d*a = [2,034758, 1,017449]
f(x + d*b) = 0,000001
x + d*b = [2,034758, 1,017449]
***************************************
Iteration 241
Gradient at x = (2,034758, 1,017449) is: (0,000112, -0,000560).
Optimal stepsize is between: [0,107392, 0,107436]
f(x + d*a) = 0,000001
x + d*a = [2,034770, 1,017389]
f(x + d*b) = 0,000001
x + d*b = [2,034770, 1,017389]
***************************************
Iteration 242
Gradient at x = (2,034770, 1,017389) is: (-0,000153, -0,000031).
Optimal stepsize is between: [1,428148, 1,428192]
f(x + d*a) = 0,000001
x + d*a = [2,034551, 1,017345]
f(x + d*b) = 0,000001
x + d*b = [2,034551, 1,017345]
***************************************
Iteration 243
Gradient at x = (2,034551, 1,017345) is: (0,000111, -0,000552).
Optimal stepsize is between: [0,107348, 0,107392]
f(x + d*a) = 0,000001
x + d*a = [2,034563, 1,017285]
f(x + d*b) = 0,000001
x + d*b = [2,034563, 1,017285]
***************************************
Iteration 244
Gradient at x = (2,034563, 1,017285) is: (-0,000150, -0,000030).
Optimal stepsize is between: [1,433637, 1,433681]
f(x + d*a) = 0,000001
x + d*a = [2,034348, 1,017242]
f(x + d*b) = 0,000001
x + d*b = [2,034348, 1,017242]
***************************************
Iteration 245
Gradient at x = (2,034348, 1,017242) is: (0,000110, -0,000543).
Optimal stepsize is between: [0,107305, 0,107375]
f(x + d*a) = 0,000001
x + d*a = [2,034360, 1,017184]
f(x + d*b) = 0,000001
x + d*b = [2,034360, 1,017184]
***************************************
Iteration 246
Gradient at x = (2,034360, 1,017184) is: (-0,000147, -0,000030).
Optimal stepsize is between: [1,444502, 1,444545]
f(x + d*a) = 0,000001
x + d*a = [2,034147, 1,017141]
f(x + d*b) = 0,000001
x + d*b = [2,034147, 1,017141]
***************************************
Iteration 247
Gradient at x = (2,034147, 1,017141) is: (0,000109, -0,000536).
Optimal stepsize is between: [0,107234, 0,107305]
f(x + d*a) = 0,000001
x + d*a = [2,034159, 1,017083]
f(x + d*b) = 0,000001
x + d*b = [2,034159, 1,017083]
***************************************
Iteration 248
Gradient at x = (2,034159, 1,017083) is: (-0,000145, -0,000030).
Optimal stepsize is between: [1,460027, 1,460071]
f(x + d*a) = 0,000001
x + d*a = [2,033948, 1,017040]
f(x + d*b) = 0,000001
x + d*b = [2,033948, 1,017040]
***************************************
Iteration 249
Gradient at x = (2,033948, 1,017040) is: (0,000108, -0,000529).
Optimal stepsize is between: [0,107163, 0,107207]
f(x + d*a) = 0,000001
x + d*a = [2,033959, 1,016983]
f(x + d*b) = 0,000001
x + d*b = [2,033959, 1,016983]
***************************************
Iteration 250
Gradient at x = (2,033959, 1,016983) is: (-0,000142, -0,000029).
Optimal stepsize is between: [1,471376, 1,471420]
f(x + d*a) = 0,000001
x + d*a = [2,033750, 1,016940]
f(x + d*b) = 0,000001
x + d*b = [2,033750, 1,016940]
***************************************
Iteration 251
Gradient at x = (2,033750, 1,016940) is: (0,000107, -0,000522).
Optimal stepsize is between: [0,107093, 0,107163]
f(x + d*a) = 0,000001
x + d*a = [2,033762, 1,016884]
f(x + d*b) = 0,000001
x + d*b = [2,033762, 1,016884]
***************************************
Iteration 252
Gradient at x = (2,033762, 1,016884) is: (-0,000140, -0,000029).
Optimal stepsize is between: [1,487457, 1,487501]
f(x + d*a) = 0,000001
x + d*a = [2,033554, 1,016842]
f(x + d*b) = 0,000001
x + d*b = [2,033554, 1,016842]
***************************************
Iteration 253
Gradient at x = (2,033554, 1,016842) is: (0,000107, -0,000515).
Optimal stepsize is between: [0,107022, 0,107065]
f(x + d*a) = 0,000001
x + d*a = [2,033566, 1,016786]
f(x + d*b) = 0,000001
x + d*b = [2,033566, 1,016786]
***************************************
Iteration 254
Gradient at x = (2,033566, 1,016786) is: (-0,000137, -0,000028).
Optimal stepsize is between: [1,499291, 1,499361]
f(x + d*a) = 0,000001
x + d*a = [2,033360, 1,016744]
f(x + d*b) = 0,000001
x + d*b = [2,033360, 1,016744]
***************************************
Iteration 255
Gradient at x = (2,033360, 1,016744) is: (0,000106, -0,000508).
Optimal stepsize is between: [0,106978, 0,107022]
f(x + d*a) = 0,000001
x + d*a = [2,033371, 1,016689]
f(x + d*b) = 0,000001
x + d*b = [2,033371, 1,016689]
***************************************
Iteration 256
Gradient at x = (2,033371, 1,016689) is: (-0,000135, -0,000028).
Optimal stepsize is between: [1,505292, 1,505362]
f(x + d*a) = 0,000001
x + d*a = [2,033169, 1,016647]
f(x + d*b) = 0,000001
x + d*b = [2,033169, 1,016647]
***************************************
Iteration 257
Gradient at x = (2,033169, 1,016647) is: (0,000104, -0,000501).
Optimal stepsize is between: [0,106934, 0,107005]
f(x + d*a) = 0,000001
x + d*a = [2,033180, 1,016593]
f(x + d*b) = 0,000001
x + d*b = [2,033180, 1,016593]
***************************************
Iteration 258
Gradient at x = (2,033180, 1,016593) is: (-0,000132, -0,000028).
Optimal stepsize is between: [1,517354, 1,517398]
f(x + d*a) = 0,000001
x + d*a = [2,032979, 1,016551]
f(x + d*b) = 0,000001
x + d*b = [2,032979, 1,016551]
***************************************
Iteration 259
Gradient at x = (2,032979, 1,016551) is: (0,000104, -0,000494).
Optimal stepsize is between: [0,106864, 0,106934]
f(x + d*a) = 0,000001
x + d*a = [2,032990, 1,016499]
f(x + d*b) = 0,000001
x + d*b = [2,032990, 1,016499]
***************************************
Iteration 260
Gradient at x = (2,032990, 1,016499) is: (-0,000130, -0,000027).
Optimal stepsize is between: [1,534562, 1,534606]
f(x + d*a) = 0,000001
x + d*a = [2,032791, 1,016457]
f(x + d*b) = 0,000001
x + d*b = [2,032791, 1,016457]
***************************************
Iteration 261
Gradient at x = (2,032791, 1,016457) is: (0,000103, -0,000488).
Optimal stepsize is between: [0,106793, 0,106837]
f(x + d*a) = 0,000001
x + d*a = [2,032802, 1,016404]
f(x + d*b) = 0,000001
x + d*b = [2,032802, 1,016404]
***************************************
Iteration 262
Gradient at x = (2,032802, 1,016404) is: (-0,000128, -0,000027).
Optimal stepsize is between: [1,547136, 1,547207]
f(x + d*a) = 0,000001
x + d*a = [2,032605, 1,016362]
f(x + d*b) = 0,000001
x + d*b = [2,032605, 1,016362]
***************************************
Iteration 263
Gradient at x = (2,032605, 1,016362) is: (0,000102, -0,000482).
Optimal stepsize is between: [0,106722, 0,106793]
f(x + d*a) = 0,000001
x + d*a = [2,032615, 1,016311]
f(x + d*b) = 0,000001
x + d*b = [2,032615, 1,016311]
***************************************
Iteration 264
Gradient at x = (2,032615, 1,016311) is: (-0,000125, -0,000027).
Optimal stepsize is between: [1,565058, 1,565102]
f(x + d*a) = 0,000001
x + d*a = [2,032419, 1,016269]
f(x + d*b) = 0,000001
x + d*b = [2,032419, 1,016269]
***************************************
Iteration 265
Gradient at x = (2,032419, 1,016269) is: (0,000102, -0,000476).
Optimal stepsize is between: [0,106635, 0,106706]
f(x + d*a) = 0,000001
x + d*a = [2,032430, 1,016218]
f(x + d*b) = 0,000001
x + d*b = [2,032430, 1,016218]
***************************************
Iteration 266
Gradient at x = (2,032430, 1,016218) is: (-0,000123, -0,000027).
Optimal stepsize is between: [1,585501, 1,585572]
f(x + d*a) = 0,000001
x + d*a = [2,032235, 1,016176]
f(x + d*b) = 0,000001
x + d*b = [2,032235, 1,016176]
***************************************
Iteration 267
Gradient at x = (2,032235, 1,016176) is: (0,000101, -0,000471).
Optimal stepsize is between: [0,106564, 0,106608]
f(x + d*a) = 0,000001
x + d*a = [2,032246, 1,016126]
f(x + d*b) = 0,000001
x + d*b = [2,032246, 1,016126]
***************************************
Iteration 268
Gradient at x = (2,032246, 1,016126) is: (-0,000121, -0,000026).
Optimal stepsize is between: [1,596938, 1,596981]
f(x + d*a) = 0,000001
x + d*a = [2,032052, 1,016084]
f(x + d*b) = 0,000001
x + d*b = [2,032052, 1,016084]
***************************************
Iteration 269
Gradient at x = (2,032052, 1,016084) is: (0,000101, -0,000465).
Optimal stepsize is between: [0,106493, 0,106564]
f(x + d*a) = 0,000001
x + d*a = [2,032063, 1,016035]
f(x + d*b) = 0,000001
x + d*b = [2,032063, 1,016035]
***************************************
Iteration 270
Gradient at x = (2,032063, 1,016035) is: (-0,000119, -0,000026).
Optimal stepsize is between: [1,618138, 1,618181]
f(x + d*a) = 0,000001
x + d*a = [2,031871, 1,015993]
f(x + d*b) = 0,000001
x + d*b = [2,031871, 1,015993]
***************************************
Iteration 271
Gradient at x = (2,031871, 1,015993) is: (0,000100, -0,000460).
Optimal stepsize is between: [0,106423, 0,106466]
f(x + d*a) = 0,000001
x + d*a = [2,031881, 1,015944]
f(x + d*b) = 0,000001
x + d*b = [2,031881, 1,015944]
***************************************
Iteration 272
Gradient at x = (2,031881, 1,015944) is: (-0,000117, -0,000026).
Optimal stepsize is between: [1,630183, 1,630254]
f(x + d*a) = 0,000001
x + d*a = [2,031691, 1,015902]
f(x + d*b) = 0,000001
x + d*b = [2,031691, 1,015902]
***************************************
Iteration 273
Gradient at x = (2,031691, 1,015902) is: (0,000099, -0,000453).
Optimal stepsize is between: [0,106379, 0,106423]
f(x + d*a) = 0,000001
x + d*a = [2,031702, 1,015854]
f(x + d*b) = 0,000001
x + d*b = [2,031702, 1,015854]
***************************************
Iteration 274
Gradient at x = (2,031702, 1,015854) is: (-0,000115, -0,000025).
Optimal stepsize is between: [1,639365, 1,639435]
f(x + d*a) = 0,000001
x + d*a = [2,031513, 1,015813]
f(x + d*b) = 0,000001
x + d*b = [2,031513, 1,015813]
***************************************
Iteration 275
Gradient at x = (2,031513, 1,015813) is: (0,000098, -0,000447).
Optimal stepsize is between: [0,106335, 0,106406]
f(x + d*a) = 0,000001
x + d*a = [2,031524, 1,015765]
f(x + d*b) = 0,000001
x + d*b = [2,031524, 1,015765]
***************************************
Iteration 276
Gradient at x = (2,031524, 1,015765) is: (-0,000113, -0,000025).
Optimal stepsize is between: [1,651727, 1,651770]
f(x + d*a) = 0,000001
x + d*a = [2,031337, 1,015724]
f(x + d*b) = 0,000001
x + d*b = [2,031337, 1,015724]
***************************************
Iteration 277
Gradient at x = (2,031337, 1,015724) is: (0,000097, -0,000441).
Optimal stepsize is between: [0,106265, 0,106335]
f(x + d*a) = 0,000001
x + d*a = [2,031348, 1,015677]
f(x + d*b) = 0,000001
x + d*b = [2,031348, 1,015677]
***************************************
Iteration 278
Gradient at x = (2,031348, 1,015677) is: (-0,000111, -0,000025).
Optimal stepsize is between: [1,674381, 1,674424]
f(x + d*a) = 0,000001
x + d*a = [2,031162, 1,015636]
f(x + d*b) = 0,000001
x + d*b = [2,031162, 1,015636]
***************************************
Iteration 279
Gradient at x = (2,031162, 1,015636) is: (0,000097, -0,000437).
Optimal stepsize is between: [0,106194, 0,106238]
f(x + d*a) = 0,000001
x + d*a = [2,031172, 1,015589]
f(x + d*b) = 0,000001
x + d*b = [2,031172, 1,015589]
***************************************
Iteration 280
Gradient at x = (2,031172, 1,015589) is: (-0,000109, -0,000024).
Optimal stepsize is between: [1,687412, 1,687483]
f(x + d*a) = 0,000001
x + d*a = [2,030989, 1,015548]
f(x + d*b) = 0,000001
x + d*b = [2,030989, 1,015548]
***************************************
Iteration 281
Gradient at x = (2,030989, 1,015548) is: (0,000096, -0,000431).
Optimal stepsize is between: [0,106123, 0,106194]
f(x + d*a) = 0,000001
x + d*a = [2,030999, 1,015502]
f(x + d*b) = 0,000001
x + d*b = [2,030999, 1,015502]
***************************************
Iteration 282
Gradient at x = (2,030999, 1,015502) is: (-0,000107, -0,000024).
Optimal stepsize is between: [1,711079, 1,711123]
f(x + d*a) = 0,000001
x + d*a = [2,030816, 1,015461]
f(x + d*b) = 0,000001
x + d*b = [2,030816, 1,015461]
***************************************
Iteration 283
Gradient at x = (2,030816, 1,015461) is: (0,000096, -0,000426).
Optimal stepsize is between: [0,106036, 0,106106]
f(x + d*a) = 0,000001
x + d*a = [2,030826, 1,015416]
f(x + d*b) = 0,000001
x + d*b = [2,030826, 1,015416]
***************************************
Iteration 284
Gradient at x = (2,030826, 1,015416) is: (-0,000105, -0,000024).
Optimal stepsize is between: [1,733548, 1,733619]
f(x + d*a) = 0,000001
x + d*a = [2,030643, 1,015374]
f(x + d*b) = 0,000001
x + d*b = [2,030643, 1,015374]
***************************************
Iteration 285
Gradient at x = (2,030643, 1,015374) is: (0,000096, -0,000422).
Optimal stepsize is between: [0,105965, 0,106036]
f(x + d*a) = 0,000001
x + d*a = [2,030654, 1,015330]
f(x + d*b) = 0,000001
x + d*b = [2,030654, 1,015330]
***************************************
Iteration 286
Gradient at x = (2,030654, 1,015330) is: (-0,000103, -0,000024).
Optimal stepsize is between: [1,749586, 1,749656]
f(x + d*a) = 0,000001
x + d*a = [2,030473, 1,015288]
f(x + d*b) = 0,000001
x + d*b = [2,030473, 1,015288]
***************************************
Iteration 287
Gradient at x = (2,030473, 1,015288) is: (0,000095, -0,000417).
Optimal stepsize is between: [0,105894, 0,105965]
f(x + d*a) = 0,000001
x + d*a = [2,030483, 1,015244]
f(x + d*b) = 0,000001
x + d*b = [2,030483, 1,015244]
***************************************
Iteration 288
Gradient at x = (2,030483, 1,015244) is: (-0,000102, -0,000023).
Optimal stepsize is between: [1,773024, 1,773095]
f(x + d*a) = 0,000001
x + d*a = [2,030303, 1,015203]
f(x + d*b) = 0,000001
x + d*b = [2,030303, 1,015203]
***************************************
Iteration 289
Gradient at x = (2,030303, 1,015203) is: (0,000095, -0,000412).
Optimal stepsize is between: [0,105824, 0,105867]
f(x + d*a) = 0,000001
x + d*a = [2,030313, 1,015159]
f(x + d*b) = 0,000001
x + d*b = [2,030313, 1,015159]
***************************************
Iteration 290
Gradient at x = (2,030313, 1,015159) is: (-0,000100, -0,000023).
Optimal stepsize is between: [1,789933, 1,790003]
f(x + d*a) = 0,000001
x + d*a = [2,030134, 1,015118]
f(x + d*b) = 0,000001
x + d*b = [2,030134, 1,015118]
***************************************
Iteration 291
Gradient at x = (2,030134, 1,015118) is: (0,000094, -0,000407).
Optimal stepsize is between: [0,105753, 0,105824]
f(x + d*a) = 0,000001
x + d*a = [2,030144, 1,015075]
f(x + d*b) = 0,000001
x + d*b = [2,030144, 1,015075]
***************************************
Iteration 292
Gradient at x = (2,030144, 1,015075) is: (-0,000098, -0,000023).
Optimal stepsize is between: [1,814384, 1,814455]
f(x + d*a) = 0,000001
x + d*a = [2,029966, 1,015033]
f(x + d*b) = 0,000001
x + d*b = [2,029966, 1,015033]
***************************************
Iteration 293
Gradient at x = (2,029966, 1,015033) is: (0,000094, -0,000403).
Optimal stepsize is between: [0,105666, 0,105736]
f(x + d*a) = 0,000001
x + d*a = [2,029976, 1,014991]
f(x + d*b) = 0,000001
x + d*b = [2,029976, 1,014991]
***************************************
Iteration 294
Gradient at x = (2,029976, 1,014991) is: (-0,000096, -0,000023).
Optimal stepsize is between: [1,842342, 1,842413]
f(x + d*a) = 0,000001
x + d*a = [2,029798, 1,014949]
f(x + d*b) = 0,000001
x + d*b = [2,029798, 1,014949]
***************************************
Iteration 295
Gradient at x = (2,029798, 1,014949) is: (0,000094, -0,000399).
Optimal stepsize is between: [0,105595, 0,105639]
f(x + d*a) = 0,000001
x + d*a = [2,029808, 1,014907]
f(x + d*b) = 0,000001
x + d*b = [2,029808, 1,014907]
***************************************
Iteration 296
Gradient at x = (2,029808, 1,014907) is: (-0,000095, -0,000022).
Optimal stepsize is between: [1,857982, 1,858026]
f(x + d*a) = 0,000001
x + d*a = [2,029632, 1,014865]
f(x + d*b) = 0,000001
x + d*b = [2,029632, 1,014865]
***************************************
Iteration 297
Gradient at x = (2,029632, 1,014865) is: (0,000093, -0,000394).
Optimal stepsize is between: [0,105524, 0,105595]
f(x + d*a) = 0,000001
x + d*a = [2,029642, 1,014824]
f(x + d*b) = 0,000001
x + d*b = [2,029642, 1,014824]
***************************************
Iteration 298
Gradient at x = (2,029642, 1,014824) is: (-0,000093, -0,000022).
Optimal stepsize is between: [1,887209, 1,887253]
f(x + d*a) = 0,000001
x + d*a = [2,029466, 1,014782]
f(x + d*b) = 0,000001
x + d*b = [2,029466, 1,014782]
***************************************
Iteration 299
Gradient at x = (2,029466, 1,014782) is: (0,000093, -0,000390).
Optimal stepsize is between: [0,105453, 0,105497]
f(x + d*a) = 0,000001
x + d*a = [2,029476, 1,014741]
f(x + d*b) = 0,000001
x + d*b = [2,029476, 1,014741]
***************************************
Iteration 300
Gradient at x = (2,029476, 1,014741) is: (-0,000092, -0,000022).
Optimal stepsize is between: [1,903862, 1,903906]
f(x + d*a) = 0,000001
x + d*a = [2,029302, 1,014699]
f(x + d*b) = 0,000001
x + d*b = [2,029302, 1,014699]
***************************************
Iteration 301
Gradient at x = (2,029302, 1,014699) is: (0,000092, -0,000386).
Optimal stepsize is between: [0,105410, 0,105453]
f(x + d*a) = 0,000001
x + d*a = [2,029311, 1,014658]
f(x + d*b) = 0,000001
x + d*b = [2,029311, 1,014658]
***************************************
Iteration 302
Gradient at x = (2,029311, 1,014658) is: (-0,000090, -0,000022).
Optimal stepsize is between: [1,916480, 1,916524]
f(x + d*a) = 0,000001
x + d*a = [2,029139, 1,014617]
f(x + d*b) = 0,000001
x + d*b = [2,029139, 1,014617]
***************************************
Iteration 303
Gradient at x = (2,029139, 1,014617) is: (0,000091, -0,000380).
Optimal stepsize is between: [0,105366, 0,105437]
f(x + d*a) = 0,000001
x + d*a = [2,029149, 1,014577]
f(x + d*b) = 0,000001
x + d*b = [2,029149, 1,014577]
***************************************
Iteration 304
Gradient at x = (2,029149, 1,014577) is: (-0,000088, -0,000021).
Optimal stepsize is between: [1,933645, 1,933716]
f(x + d*a) = 0,000001
x + d*a = [2,028978, 1,014536]
f(x + d*b) = 0,000001
x + d*b = [2,028978, 1,014536]
***************************************
Iteration 305
Gradient at x = (2,028978, 1,014536) is: (0,000090, -0,000376).
Optimal stepsize is between: [0,105295, 0,105366]
f(x + d*a) = 0,000001
x + d*a = [2,028987, 1,014496]
f(x + d*b) = 0,000001
x + d*b = [2,028987, 1,014496]
***************************************
Iteration 306
Gradient at x = (2,028987, 1,014496) is: (-0,000087, -0,000021).
Optimal stepsize is between: [1,965295, 1,965339]
f(x + d*a) = 0,000001
x + d*a = [2,028816, 1,014455]
f(x + d*b) = 0,000001
x + d*b = [2,028816, 1,014455]
***************************************
Iteration 307
Gradient at x = (2,028816, 1,014455) is: (0,000090, -0,000372).
Optimal stepsize is between: [0,105225, 0,105268]
f(x + d*a) = 0,000001
x + d*a = [2,028826, 1,014416]
f(x + d*b) = 0,000001
x + d*b = [2,028826, 1,014416]
***************************************
Iteration 308
Gradient at x = (2,028826, 1,014416) is: (-0,000085, -0,000021).
Optimal stepsize is between: [1,983517, 1,983560]
f(x + d*a) = 0,000001
x + d*a = [2,028657, 1,014374]
f(x + d*b) = 0,000001
x + d*b = [2,028657, 1,014374]
***************************************
Iteration 309
Gradient at x = (2,028657, 1,014374) is: (0,000090, -0,000368).
Optimal stepsize is between: [0,105154, 0,105225]
f(x + d*a) = 0,000001
x + d*a = [2,028666, 1,014336]
f(x + d*b) = 0,000001
x + d*b = [2,028666, 1,014336]
***************************************
Iteration 310
Gradient at x = (2,028666, 1,014336) is: (-0,000084, -0,000021).
Optimal stepsize is between: [2,016735, 2,016779]
f(x + d*a) = 0,000001
x + d*a = [2,028497, 1,014294]
f(x + d*b) = 0,000001
x + d*b = [2,028497, 1,014294]
***************************************
Iteration 311
Gradient at x = (2,028497, 1,014294) is: (0,000090, -0,000364).
Optimal stepsize is between: [0,105066, 0,105137]
f(x + d*a) = 0,000001
x + d*a = [2,028506, 1,014256]
f(x + d*b) = 0,000001
x + d*b = [2,028506, 1,014256]
***************************************
Iteration 312
Gradient at x = (2,028506, 1,014256) is: (-0,000082, -0,000020).
Optimal stepsize is between: [2,048712, 2,048783]
f(x + d*a) = 0,000001
x + d*a = [2,028337, 1,014214]
f(x + d*b) = 0,000001
x + d*b = [2,028337, 1,014214]
***************************************
Iteration 313
Gradient at x = (2,028337, 1,014214) is: (0,000090, -0,000361).
Optimal stepsize is between: [0,104996, 0,105066]
f(x + d*a) = 0,000001
x + d*a = [2,028347, 1,014176]
f(x + d*b) = 0,000001
x + d*b = [2,028347, 1,014176]
***************************************
Iteration 314
Gradient at x = (2,028347, 1,014176) is: (-0,000081, -0,000020).
Optimal stepsize is between: [2,071269, 2,071312]
f(x + d*a) = 0,000001
x + d*a = [2,028179, 1,014134]
f(x + d*b) = 0,000001
x + d*b = [2,028179, 1,014134]
***************************************
Iteration 315
Gradient at x = (2,028179, 1,014134) is: (0,000089, -0,000357).
Optimal stepsize is between: [0,104925, 0,104996]
f(x + d*a) = 0,000001
x + d*a = [2,028188, 1,014097]
f(x + d*b) = 0,000001
x + d*b = [2,028188, 1,014097]
***************************************
Iteration 316
Gradient at x = (2,028188, 1,014097) is: (-0,000080, -0,000020).
Optimal stepsize is between: [2,104928, 2,104972]
f(x + d*a) = 0,000001
x + d*a = [2,028021, 1,014055]
f(x + d*b) = 0,000001
x + d*b = [2,028021, 1,014055]
***************************************
Iteration 317
Gradient at x = (2,028021, 1,014055) is: (0,000089, -0,000354).
Optimal stepsize is between: [0,104854, 0,104898]
f(x + d*a) = 0,000001
x + d*a = [2,028030, 1,014017]
f(x + d*b) = 0,000001
x + d*b = [2,028030, 1,014017]
***************************************
Iteration 318
Gradient at x = (2,028030, 1,014017) is: (-0,000078, -0,000020).
Optimal stepsize is between: [2,128966, 2,129009]
f(x + d*a) = 0,000001
x + d*a = [2,027863, 1,013975]
f(x + d*b) = 0,000001
x + d*b = [2,027863, 1,013975]
***************************************
Iteration 319
Gradient at x = (2,027863, 1,013975) is: (0,000088, -0,000350).
Optimal stepsize is between: [0,104811, 0,104854]
f(x + d*a) = 0,000001
x + d*a = [2,027873, 1,013939]
f(x + d*b) = 0,000001
x + d*b = [2,027873, 1,013939]
***************************************
Iteration 320
Gradient at x = (2,027873, 1,013939) is: (-0,000077, -0,000019).
Optimal stepsize is between: [2,141769, 2,141812]
f(x + d*a) = 0,000001
x + d*a = [2,027708, 1,013897]
f(x + d*b) = 0,000001
x + d*b = [2,027708, 1,013897]
***************************************
Iteration 321
Gradient at x = (2,027708, 1,013897) is: (0,000087, -0,000345).
Optimal stepsize is between: [0,104767, 0,104838]
f(x + d*a) = 0,000001
x + d*a = [2,027717, 1,013861]
f(x + d*b) = 0,000001
x + d*b = [2,027717, 1,013861]
***************************************
Iteration 322
Gradient at x = (2,027717, 1,013861) is: (-0,000076, -0,000019).
Optimal stepsize is between: [2,166590, 2,166661]
f(x + d*a) = 0,000001
x + d*a = [2,027553, 1,013819]
f(x + d*b) = 0,000001
x + d*b = [2,027553, 1,013819]
***************************************
Iteration 323
Gradient at x = (2,027553, 1,013819) is: (0,000087, -0,000341).
Optimal stepsize is between: [0,104696, 0,104767]
f(x + d*a) = 0,000001
x + d*a = [2,027563, 1,013784]
f(x + d*b) = 0,000001
x + d*b = [2,027563, 1,013784]
***************************************
Iteration 324
Gradient at x = (2,027563, 1,013784) is: (-0,000074, -0,000019).
Optimal stepsize is between: [2,203501, 2,203572]
f(x + d*a) = 0,000001
x + d*a = [2,027399, 1,013742]
f(x + d*b) = 0,000001
x + d*b = [2,027399, 1,013742]
***************************************
Iteration 325
Gradient at x = (2,027399, 1,013742) is: (0,000087, -0,000338).
Optimal stepsize is between: [0,104626, 0,104669]
f(x + d*a) = 0,000001
x + d*a = [2,027408, 1,013706]
f(x + d*b) = 0,000001
x + d*b = [2,027408, 1,013706]
***************************************
Iteration 326
Gradient at x = (2,027408, 1,013706) is: (-0,000073, -0,000019).
Optimal stepsize is between: [2,230049, 2,230120]
f(x + d*a) = 0,000001
x + d*a = [2,027245, 1,013664]
f(x + d*b) = 0,000001
x + d*b = [2,027245, 1,013664]
***************************************
Iteration 327
Gradient at x = (2,027245, 1,013664) is: (0,000086, -0,000334).
Optimal stepsize is between: [0,104555, 0,104626]
f(x + d*a) = 0,000001
x + d*a = [2,027254, 1,013629]
f(x + d*b) = 0,000001
x + d*b = [2,027254, 1,013629]
***************************************
Iteration 328
Gradient at x = (2,027254, 1,013629) is: (-0,000072, -0,000019).
Optimal stepsize is between: [2,269056, 2,269127]
f(x + d*a) = 0,000001
x + d*a = [2,027092, 1,013587]
f(x + d*b) = 0,000001
x + d*b = [2,027092, 1,013587]
***************************************
Iteration 329
Gradient at x = (2,027092, 1,013587) is: (0,000086, -0,000331).
Optimal stepsize is between: [0,104467, 0,104538]
f(x + d*a) = 0,000001
x + d*a = [2,027101, 1,013553]
f(x + d*b) = 0,000001
x + d*b = [2,027101, 1,013553]
***************************************
Iteration 330
Gradient at x = (2,027101, 1,013553) is: (-0,000070, -0,000018).
Optimal stepsize is between: [2,313607, 2,313678]
f(x + d*a) = 0,000001
x + d*a = [2,026938, 1,013510]
f(x + d*b) = 0,000001
x + d*b = [2,026938, 1,013510]
***************************************
Iteration 331
Gradient at x = (2,026938, 1,013510) is: (0,000086, -0,000329).
Optimal stepsize is between: [0,104397, 0,104467]
f(x + d*a) = 0,000001
x + d*a = [2,026947, 1,013476]
f(x + d*b) = 0,000001
x + d*b = [2,026947, 1,013476]
***************************************
Iteration 332
Gradient at x = (2,026947, 1,013476) is: (-0,000069, -0,000018).
Optimal stepsize is between: [2,339115, 2,339159]
f(x + d*a) = 0,000001
x + d*a = [2,026785, 1,013433]
f(x + d*b) = 0,000001
x + d*b = [2,026785, 1,013433]
***************************************
Iteration 333
Gradient at x = (2,026785, 1,013433) is: (0,000085, -0,000325).
Optimal stepsize is between: [0,104326, 0,104397]
f(x + d*a) = 0,000001
x + d*a = [2,026794, 1,013399]
f(x + d*b) = 0,000001
x + d*b = [2,026794, 1,013399]
***************************************
Iteration 334
Gradient at x = (2,026794, 1,013399) is: (-0,000068, -0,000018).
Optimal stepsize is between: [2,386177, 2,386248]
f(x + d*a) = 0,000001
x + d*a = [2,026632, 1,013356]
f(x + d*b) = 0,000001
x + d*b = [2,026632, 1,013356]
***************************************
Iteration 335
Gradient at x = (2,026632, 1,013356) is: (0,000086, -0,000322).
Optimal stepsize is between: [0,104255, 0,104299]
f(x + d*a) = 0,000001
x + d*a = [2,026641, 1,013323]
f(x + d*b) = 0,000001
x + d*b = [2,026641, 1,013323]
***************************************
Iteration 336
Gradient at x = (2,026641, 1,013323) is: (-0,000067, -0,000018).
Optimal stepsize is between: [2,413836, 2,413879]
f(x + d*a) = 0,000000
x + d*a = [2,026480, 1,013280]
f(x + d*b) = 0,000000
x + d*b = [2,026480, 1,013280]
***************************************
Iteration 337
Gradient at x = (2,026480, 1,013280) is: (0,000085, -0,000318).
Optimal stepsize is between: [0,104185, 0,104255]
f(x + d*a) = 0,000000
x + d*a = [2,026489, 1,013246]
f(x + d*b) = 0,000000
x + d*b = [2,026489, 1,013246]
***************************************
Iteration 338
Gradient at x = (2,026489, 1,013246) is: (-0,000066, -0,000018).
Optimal stepsize is between: [2,463664, 2,463734]
f(x + d*a) = 0,000000
x + d*a = [2,026327, 1,013203]
f(x + d*b) = 0,000000
x + d*b = [2,026327, 1,013203]
***************************************
Iteration 339
Gradient at x = (2,026327, 1,013203) is: (0,000085, -0,000316).
Optimal stepsize is between: [0,104097, 0,104168]
f(x + d*a) = 0,000000
x + d*a = [2,026336, 1,013170]
f(x + d*b) = 0,000000
x + d*b = [2,026336, 1,013170]
***************************************
Iteration 340
Gradient at x = (2,026336, 1,013170) is: (-0,000064, -0,000017).
Optimal stepsize is between: [2,512822, 2,512866]
f(x + d*a) = 0,000000
x + d*a = [2,026174, 1,013126]
f(x + d*b) = 0,000000
x + d*b = [2,026174, 1,013126]
***************************************
Iteration 341
Gradient at x = (2,026174, 1,013126) is: (0,000085, -0,000313).
Optimal stepsize is between: [0,104026, 0,104070]
f(x + d*a) = 0,000000
x + d*a = [2,026183, 1,013094]
f(x + d*b) = 0,000000
x + d*b = [2,026183, 1,013094]
***************************************
Iteration 342
Gradient at x = (2,026183, 1,013094) is: (-0,000063, -0,000017).
Optimal stepsize is between: [2,546896, 2,546966]
f(x + d*a) = 0,000000
x + d*a = [2,026022, 1,013050]
f(x + d*b) = 0,000000
x + d*b = [2,026022, 1,013050]
***************************************
Iteration 343
Gradient at x = (2,026022, 1,013050) is: (0,000085, -0,000310).
Optimal stepsize is between: [0,103956, 0,104026]
f(x + d*a) = 0,000000
x + d*a = [2,026031, 1,013018]
f(x + d*b) = 0,000000
x + d*b = [2,026031, 1,013018]
***************************************
Iteration 344
Gradient at x = (2,026031, 1,013018) is: (-0,000062, -0,000017).
Optimal stepsize is between: [2,599376, 2,599447]
f(x + d*a) = 0,000000
x + d*a = [2,025870, 1,012973]
f(x + d*b) = 0,000000
x + d*b = [2,025870, 1,012973]
***************************************
Iteration 345
Gradient at x = (2,025870, 1,012973) is: (0,000085, -0,000308).
Optimal stepsize is between: [0,103885, 0,103929]
f(x + d*a) = 0,000000
x + d*a = [2,025879, 1,012941]
f(x + d*b) = 0,000000
x + d*b = [2,025879, 1,012941]
***************************************
Iteration 346
Gradient at x = (2,025879, 1,012941) is: (-0,000061, -0,000017).
Optimal stepsize is between: [2,636260, 2,636303]
f(x + d*a) = 0,000000
x + d*a = [2,025718, 1,012897]
f(x + d*b) = 0,000000
x + d*b = [2,025718, 1,012897]
***************************************
Iteration 347
Gradient at x = (2,025718, 1,012897) is: (0,000084, -0,000304).
Optimal stepsize is between: [0,103841, 0,103885]
f(x + d*a) = 0,000000
x + d*a = [2,025727, 1,012865]
f(x + d*b) = 0,000000
x + d*b = [2,025727, 1,012865]
***************************************
Iteration 348
Gradient at x = (2,025727, 1,012865) is: (-0,000060, -0,000017).
Optimal stepsize is between: [2,656676, 2,656719]
f(x + d*a) = 0,000000
x + d*a = [2,025568, 1,012821]
f(x + d*b) = 0,000000
x + d*b = [2,025568, 1,012821]
***************************************
Iteration 349
Gradient at x = (2,025568, 1,012821) is: (0,000083, -0,000300).
Optimal stepsize is between: [0,103798, 0,103868]
f(x + d*a) = 0,000000
x + d*a = [2,025576, 1,012790]
f(x + d*b) = 0,000000
x + d*b = [2,025576, 1,012790]
***************************************
Iteration 350
Gradient at x = (2,025576, 1,012790) is: (-0,000059, -0,000016).
Optimal stepsize is between: [2,695111, 2,695182]
f(x + d*a) = 0,000000
x + d*a = [2,025418, 1,012746]
f(x + d*b) = 0,000000
x + d*b = [2,025418, 1,012746]
***************************************
Iteration 351
Gradient at x = (2,025418, 1,012746) is: (0,000083, -0,000297).
Optimal stepsize is between: [0,103727, 0,103798]
f(x + d*a) = 0,000000
x + d*a = [2,025427, 1,012715]
f(x + d*b) = 0,000000
x + d*b = [2,025427, 1,012715]
***************************************
Iteration 352
Gradient at x = (2,025427, 1,012715) is: (-0,000058, -0,000016).
Optimal stepsize is between: [2,753936, 2,754006]
f(x + d*a) = 0,000000
x + d*a = [2,025268, 1,012671]
f(x + d*b) = 0,000000
x + d*b = [2,025268, 1,012671]
***************************************
Iteration 353
Gradient at x = (2,025268, 1,012671) is: (0,000083, -0,000295).
Optimal stepsize is between: [0,103656, 0,103700]
f(x + d*a) = 0,000000
x + d*a = [2,025276, 1,012640]
f(x + d*b) = 0,000000
x + d*b = [2,025276, 1,012640]
***************************************
Iteration 354
Gradient at x = (2,025276, 1,012640) is: (-0,000057, -0,000016).
Optimal stepsize is between: [2,795780, 2,795851]
f(x + d*a) = 0,000000
x + d*a = [2,025118, 1,012596]
f(x + d*b) = 0,000000
x + d*b = [2,025118, 1,012596]
***************************************
Iteration 355
Gradient at x = (2,025118, 1,012596) is: (0,000082, -0,000291).
Optimal stepsize is between: [0,103586, 0,103656]
f(x + d*a) = 0,000000
x + d*a = [2,025127, 1,012565]
f(x + d*b) = 0,000000
x + d*b = [2,025127, 1,012565]
***************************************
Iteration 356
Gradient at x = (2,025127, 1,012565) is: (-0,000056, -0,000016).
Optimal stepsize is between: [2,858869, 2,858913]
f(x + d*a) = 0,000000
x + d*a = [2,024968, 1,012520]
f(x + d*b) = 0,000000
x + d*b = [2,024968, 1,012520]
***************************************
Iteration 357
Gradient at x = (2,024968, 1,012520) is: (0,000082, -0,000289).
Optimal stepsize is between: [0,103498, 0,103569]
f(x + d*a) = 0,000000
x + d*a = [2,024976, 1,012490]
f(x + d*b) = 0,000000
x + d*b = [2,024976, 1,012490]
***************************************
Iteration 358
Gradient at x = (2,024976, 1,012490) is: (-0,000054, -0,000016).
Optimal stepsize is between: [2,930796, 2,930840]
f(x + d*a) = 0,000000
x + d*a = [2,024817, 1,012444]
f(x + d*b) = 0,000000
x + d*b = [2,024817, 1,012444]
***************************************
Iteration 359
Gradient at x = (2,024817, 1,012444) is: (0,000083, -0,000287).
Optimal stepsize is between: [0,103427, 0,103498]
f(x + d*a) = 0,000000
x + d*a = [2,024825, 1,012415]
f(x + d*b) = 0,000000
x + d*b = [2,024825, 1,012415]
***************************************
Iteration 360
Gradient at x = (2,024825, 1,012415) is: (-0,000053, -0,000015).
Optimal stepsize is between: [2,972967, 2,973011]
f(x + d*a) = 0,000000
x + d*a = [2,024666, 1,012369]
f(x + d*b) = 0,000000
x + d*b = [2,024666, 1,012369]
***************************************
Iteration 361
Gradient at x = (2,024666, 1,012369) is: (0,000082, -0,000284).
Optimal stepsize is between: [0,103357, 0,103427]
f(x + d*a) = 0,000000
x + d*a = [2,024675, 1,012339]
f(x + d*b) = 0,000000
x + d*b = [2,024675, 1,012339]
***************************************
Iteration 362
Gradient at x = (2,024675, 1,012339) is: (-0,000052, -0,000015).
Optimal stepsize is between: [3,050340, 3,050410]
f(x + d*a) = 0,000000
x + d*a = [2,024515, 1,012293]
f(x + d*b) = 0,000000
x + d*b = [2,024515, 1,012293]
***************************************
Iteration 363
Gradient at x = (2,024515, 1,012293) is: (0,000082, -0,000282).
Optimal stepsize is between: [0,103286, 0,103330]
f(x + d*a) = 0,000000
x + d*a = [2,024523, 1,012264]
f(x + d*b) = 0,000000
x + d*b = [2,024523, 1,012263]
***************************************
Iteration 364
Gradient at x = (2,024523, 1,012264) is: (-0,000051, -0,000015).
Optimal stepsize is between: [3,096933, 3,096977]
f(x + d*a) = 0,000000
x + d*a = [2,024364, 1,012217]
f(x + d*b) = 0,000000
x + d*b = [2,024364, 1,012217]
***************************************
Iteration 365
Gradient at x = (2,024364, 1,012217) is: (0,000082, -0,000279).
Optimal stepsize is between: [0,103242, 0,103286]
f(x + d*a) = 0,000000
x + d*a = [2,024372, 1,012188]
f(x + d*b) = 0,000000
x + d*b = [2,024372, 1,012188]
***************************************
Iteration 366
Gradient at x = (2,024372, 1,012188) is: (-0,000050, -0,000015).
Optimal stepsize is between: [3,130337, 3,130408]
f(x + d*a) = 0,000000
x + d*a = [2,024214, 1,012142]
f(x + d*b) = 0,000000
x + d*b = [2,024214, 1,012142]
***************************************
Iteration 367
Gradient at x = (2,024214, 1,012142) is: (0,000081, -0,000275).
Optimal stepsize is between: [0,103199, 0,103269]
f(x + d*a) = 0,000000
x + d*a = [2,024223, 1,012113]
f(x + d*b) = 0,000000
x + d*b = [2,024223, 1,012113]
***************************************
Iteration 368
Gradient at x = (2,024223, 1,012113) is: (-0,000050, -0,000015).
Optimal stepsize is between: [3,179381, 3,179425]
f(x + d*a) = 0,000000
x + d*a = [2,024065, 1,012067]
f(x + d*b) = 0,000000
x + d*b = [2,024065, 1,012067]
***************************************
Iteration 369
Gradient at x = (2,024065, 1,012067) is: (0,000080, -0,000272).
Optimal stepsize is between: [0,103128, 0,103199]
f(x + d*a) = 0,000000
x + d*a = [2,024073, 1,012039]
f(x + d*b) = 0,000000
x + d*b = [2,024073, 1,012039]
***************************************
Iteration 370
Gradient at x = (2,024073, 1,012039) is: (-0,000049, -0,000014).
Optimal stepsize is between: [3,267618, 3,267661]
f(x + d*a) = 0,000000
x + d*a = [2,023915, 1,011991]
f(x + d*b) = 0,000000
x + d*b = [2,023915, 1,011991]
***************************************
Iteration 371
Gradient at x = (2,023915, 1,011991) is: (0,000081, -0,000271).
Optimal stepsize is between: [0,103057, 0,103101]
f(x + d*a) = 0,000000
x + d*a = [2,023923, 1,011963]
f(x + d*b) = 0,000000
x + d*b = [2,023923, 1,011963]
***************************************
Iteration 372
Gradient at x = (2,023923, 1,011963) is: (-0,000048, -0,000014).
Optimal stepsize is between: [3,322107, 3,322178]
f(x + d*a) = 0,000000
x + d*a = [2,023765, 1,011916]
f(x + d*b) = 0,000000
x + d*b = [2,023765, 1,011916]
***************************************
Iteration 373
Gradient at x = (2,023765, 1,011916) is: (0,000080, -0,000268).
Optimal stepsize is between: [0,102986, 0,103057]
f(x + d*a) = 0,000000
x + d*a = [2,023773, 1,011888]
f(x + d*b) = 0,000000
x + d*b = [2,023773, 1,011888]
***************************************
Iteration 374
Gradient at x = (2,023773, 1,011888) is: (-0,000047, -0,000014).
Optimal stepsize is between: [3,417957, 3,418028]
f(x + d*a) = 0,000000
x + d*a = [2,023614, 1,011840]
f(x + d*b) = 0,000000
x + d*b = [2,023614, 1,011840]
***************************************
Iteration 375
Gradient at x = (2,023614, 1,011840) is: (0,000081, -0,000266).
Optimal stepsize is between: [0,102916, 0,102959]
f(x + d*a) = 0,000000
x + d*a = [2,023622, 1,011813]
f(x + d*b) = 0,000000
x + d*b = [2,023622, 1,011813]
***************************************
Iteration 376
Gradient at x = (2,023622, 1,011813) is: (-0,000046, -0,000014).
Optimal stepsize is between: [3,478720, 3,478791]
f(x + d*a) = 0,000000
x + d*a = [2,023463, 1,011764]
f(x + d*b) = 0,000000
x + d*b = [2,023463, 1,011764]
***************************************
Iteration 377
Gradient at x = (2,023463, 1,011764) is: (0,000080, -0,000264).
Optimal stepsize is between: [0,102872, 0,102916]
f(x + d*a) = 0,000000
x + d*a = [2,023471, 1,011737]
f(x + d*b) = 0,000000
x + d*b = [2,023471, 1,011737]
***************************************
Iteration 378
Gradient at x = (2,023471, 1,011737) is: (-0,000045, -0,000014).
Optimal stepsize is between: [3,519481, 3,519552]
f(x + d*a) = 0,000000
x + d*a = [2,023313, 1,011689]
f(x + d*b) = 0,000000
x + d*b = [2,023313, 1,011689]
***************************************
Iteration 379
Gradient at x = (2,023313, 1,011689) is: (0,000079, -0,000260).
Optimal stepsize is between: [0,102828, 0,102899]
f(x + d*a) = 0,000000
x + d*a = [2,023321, 1,011662]
f(x + d*b) = 0,000000
x + d*b = [2,023321, 1,011662]
***************************************
Iteration 380
Gradient at x = (2,023321, 1,011662) is: (-0,000044, -0,000013).
Optimal stepsize is between: [3,583610, 3,583680]
f(x + d*a) = 0,000000
x + d*a = [2,023163, 1,011614]
f(x + d*b) = 0,000000
x + d*b = [2,023163, 1,011614]
***************************************
Iteration 381
Gradient at x = (2,023163, 1,011614) is: (0,000079, -0,000257).
Optimal stepsize is between: [0,102758, 0,102828]
f(x + d*a) = 0,000000
x + d*a = [2,023172, 1,011587]
f(x + d*b) = 0,000000
x + d*b = [2,023172, 1,011587]
***************************************
Iteration 382
Gradient at x = (2,023172, 1,011587) is: (-0,000043, -0,000013).
Optimal stepsize is between: [3,695258, 3,695301]
f(x + d*a) = 0,000000
x + d*a = [2,023012, 1,011538]
f(x + d*b) = 0,000000
x + d*b = [2,023012, 1,011538]
***************************************
Iteration 383
Gradient at x = (2,023012, 1,011538) is: (0,000079, -0,000256).
Optimal stepsize is between: [0,102687, 0,102731]
f(x + d*a) = 0,000000
x + d*a = [2,023020, 1,011512]
f(x + d*b) = 0,000000
x + d*b = [2,023020, 1,011512]
***************************************
Iteration 384
Gradient at x = (2,023020, 1,011512) is: (-0,000042, -0,000013).
Optimal stepsize is between: [3,767282, 3,767353]
f(x + d*a) = 0,000000
x + d*a = [2,022861, 1,011462]
f(x + d*b) = 0,000000
x + d*b = [2,022861, 1,011462]
***************************************
Iteration 385
Gradient at x = (2,022861, 1,011462) is: (0,000079, -0,000253).
Optimal stepsize is between: [0,102616, 0,102687]
f(x + d*a) = 0,000000
x + d*a = [2,022869, 1,011436]
f(x + d*b) = 0,000000
x + d*b = [2,022869, 1,011436]
***************************************
Iteration 386
Gradient at x = (2,022869, 1,011436) is: (-0,000041, -0,000013).
Optimal stepsize is between: [3,889979, 3,890023]
f(x + d*a) = 0,000000
x + d*a = [2,022709, 1,011386]
f(x + d*b) = 0,000000
x + d*b = [2,022709, 1,011386]
***************************************
Iteration 387
Gradient at x = (2,022709, 1,011386) is: (0,000079, -0,000252).
Optimal stepsize is between: [0,102529, 0,102600]
f(x + d*a) = 0,000000
x + d*a = [2,022717, 1,011360]
f(x + d*b) = 0,000000
x + d*b = [2,022717, 1,011360]
***************************************
Iteration 388
Gradient at x = (2,022717, 1,011360) is: (-0,000040, -0,000013).
Optimal stepsize is between: [4,021515, 4,021558]
f(x + d*a) = 0,000000
x + d*a = [2,022554, 1,011308]
f(x + d*b) = 0,000000
x + d*b = [2,022554, 1,011308]
***************************************
Iteration 389
Gradient at x = (2,022554, 1,011308) is: (0,000080, -0,000251).
Optimal stepsize is between: [0,102458, 0,102529]
f(x + d*a) = 0,000000
x + d*a = [2,022562, 1,011283]
f(x + d*b) = 0,000000
x + d*b = [2,022562, 1,011283]
***************************************
Iteration 390
Gradient at x = (2,022562, 1,011283) is: (-0,000040, -0,000013).
Optimal stepsize is between: [4,108412, 4,108455]
f(x + d*a) = 0,000000
x + d*a = [2,022399, 1,011231]
f(x + d*b) = 0,000000
x + d*b = [2,022399, 1,011231]
***************************************
Iteration 391
Gradient at x = (2,022399, 1,011231) is: (0,000079, -0,000248).
Optimal stepsize is between: [0,102387, 0,102458]
f(x + d*a) = 0,000000
x + d*a = [2,022407, 1,011205]
f(x + d*b) = 0,000000
x + d*b = [2,022407, 1,011205]
***************************************
Iteration 392
Gradient at x = (2,022407, 1,011205) is: (-0,000039, -0,000012).
Optimal stepsize is between: [4,254275, 4,254345]
f(x + d*a) = 0,000000
x + d*a = [2,022243, 1,011152]
f(x + d*b) = 0,000000
x + d*b = [2,022243, 1,011152]
***************************************
Iteration 393
Gradient at x = (2,022243, 1,011152) is: (0,000080, -0,000247).
Optimal stepsize is between: [0,102317, 0,102360]
f(x + d*a) = 0,000000
x + d*a = [2,022251, 1,011127]
f(x + d*b) = 0,000000
x + d*b = [2,022251, 1,011127]
***************************************
Iteration 394
Gradient at x = (2,022251, 1,011127) is: (-0,000038, -0,000012).
Optimal stepsize is between: [4,353631, 4,353702]
f(x + d*a) = 0,000000
x + d*a = [2,022086, 1,011073]
f(x + d*b) = 0,000000
x + d*b = [2,022086, 1,011073]
***************************************
Iteration 395
Gradient at x = (2,022086, 1,011073) is: (0,000079, -0,000245).
Optimal stepsize is between: [0,102273, 0,102317]
f(x + d*a) = 0,000000
x + d*a = [2,022094, 1,011048]
f(x + d*b) = 0,000000
x + d*b = [2,022094, 1,011048]
***************************************
Iteration 396
Gradient at x = (2,022094, 1,011048) is: (-0,000037, -0,000012).
Optimal stepsize is between: [4,414623, 4,414694]
f(x + d*a) = 0,000000
x + d*a = [2,021930, 1,010995]
f(x + d*b) = 0,000000
x + d*b = [2,021930, 1,010995]
***************************************
Iteration 397
Gradient at x = (2,021930, 1,010995) is: (0,000078, -0,000241).
Optimal stepsize is between: [0,102229, 0,102300]
f(x + d*a) = 0,000000
x + d*a = [2,021938, 1,010970]
f(x + d*b) = 0,000000
x + d*b = [2,021938, 1,010970]
***************************************
Iteration 398
Gradient at x = (2,021938, 1,010970) is: (-0,000036, -0,000012).
Optimal stepsize is between: [4,521038, 4,521108]
f(x + d*a) = 0,000000
x + d*a = [2,021774, 1,010917]
f(x + d*b) = 0,000000
x + d*b = [2,021774, 1,010917]
***************************************
Iteration 399
Gradient at x = (2,021774, 1,010917) is: (0,000078, -0,000239).
Optimal stepsize is between: [0,102159, 0,102229]
f(x + d*a) = 0,000000
x + d*a = [2,021782, 1,010892]
f(x + d*b) = 0,000000
x + d*b = [2,021782, 1,010892]
***************************************
Iteration 400
Gradient at x = (2,021782, 1,010892) is: (-0,000035, -0,000012).
Optimal stepsize is between: [4,697881, 4,697952]
f(x + d*a) = 0,000000
x + d*a = [2,021615, 1,010837]
f(x + d*b) = 0,000000
x + d*b = [2,021615, 1,010837]
***************************************
Iteration 401
Gradient at x = (2,021615, 1,010837) is: (0,000079, -0,000238).
Optimal stepsize is between: [0,102088, 0,102132]
f(x + d*a) = 0,000000
x + d*a = [2,021623, 1,010813]
f(x + d*b) = 0,000000
x + d*b = [2,021623, 1,010813]
***************************************
Iteration 402
Gradient at x = (2,021623, 1,010813) is: (-0,000035, -0,000012).
Optimal stepsize is between: [4,821134, 4,821204]
f(x + d*a) = 0,000000
x + d*a = [2,021456, 1,010757]
f(x + d*b) = 0,000000
x + d*b = [2,021456, 1,010757]
***************************************
Iteration 403
Gradient at x = (2,021456, 1,010757) is: (0,000078, -0,000236).
Optimal stepsize is between: [0,102017, 0,102088]
f(x + d*a) = 0,000000
x + d*a = [2,021464, 1,010733]
f(x + d*b) = 0,000000
x + d*b = [2,021464, 1,010733]
***************************************
Iteration 404
Gradient at x = (2,021464, 1,010733) is: (-0,000034, -0,000011).
Optimal stepsize is between: [5,020974, 5,021018]
f(x + d*a) = 0,000000
x + d*a = [2,021294, 1,010676]
f(x + d*b) = 0,000000
x + d*b = [2,021294, 1,010676]
***************************************
Iteration 405
Gradient at x = (2,021294, 1,010676) is: (0,000079, -0,000235).
Optimal stepsize is between: [0,101930, 0,102000]
f(x + d*a) = 0,000000
x + d*a = [2,021302, 1,010652]
f(x + d*b) = 0,000000
x + d*b = [2,021302, 1,010652]
***************************************
Iteration 406
Gradient at x = (2,021302, 1,010652) is: (-0,000033, -0,000011).
Optimal stepsize is between: [5,250483, 5,250554]
f(x + d*a) = 0,000000
x + d*a = [2,021128, 1,010593]
f(x + d*b) = 0,000000
x + d*b = [2,021128, 1,010593]
***************************************
Iteration 407
Gradient at x = (2,021128, 1,010593) is: (0,000080, -0,000235).
Optimal stepsize is between: [0,101859, 0,101930]
f(x + d*a) = 0,000000
x + d*a = [2,021136, 1,010570]
f(x + d*b) = 0,000000
x + d*b = [2,021136, 1,010570]
***************************************
Iteration 408
Gradient at x = (2,021136, 1,010570) is: (-0,000032, -0,000011).
Optimal stepsize is between: [5,396275, 5,396346]
f(x + d*a) = 0,000000
x + d*a = [2,020962, 1,010510]
f(x + d*b) = 0,000000
x + d*b = [2,020962, 1,010510]
***************************************
Iteration 409
Gradient at x = (2,020962, 1,010510) is: (0,000079, -0,000232).
Optimal stepsize is between: [0,101788, 0,101859]
f(x + d*a) = 0,000000
x + d*a = [2,020970, 1,010487]
f(x + d*b) = 0,000000
x + d*b = [2,020970, 1,010487]
***************************************
Iteration 410
Gradient at x = (2,020970, 1,010487) is: (-0,000031, -0,000011).
Optimal stepsize is between: [5,658791, 5,658862]
f(x + d*a) = 0,000000
x + d*a = [2,020792, 1,010425]
f(x + d*b) = 0,000000
x + d*b = [2,020792, 1,010425]
***************************************
Iteration 411
Gradient at x = (2,020792, 1,010425) is: (0,000080, -0,000232).
Optimal stepsize is between: [0,101718, 0,101761]
f(x + d*a) = 0,000000
x + d*a = [2,020800, 1,010402]
f(x + d*b) = 0,000000
x + d*b = [2,020801, 1,010402]
***************************************
Iteration 412
Gradient at x = (2,020800, 1,010402) is: (-0,000031, -0,000011).
Optimal stepsize is between: [5,833511, 5,833581]
f(x + d*a) = 0,000000
x + d*a = [2,020622, 1,010339]
f(x + d*b) = 0,000000
x + d*b = [2,020622, 1,010339]
***************************************
Iteration 413
Gradient at x = (2,020622, 1,010339) is: (0,000080, -0,000229).
Optimal stepsize is between: [0,101647, 0,101718]
f(x + d*a) = 0,000000
x + d*a = [2,020630, 1,010316]
f(x + d*b) = 0,000000
x + d*b = [2,020630, 1,010316]
***************************************
Iteration 414
Gradient at x = (2,020630, 1,010316) is: (-0,000030, -0,000010).
Optimal stepsize is between: [6,137413, 6,137484]
f(x + d*a) = 0,000000
x + d*a = [2,020446, 1,010252]
f(x + d*b) = 0,000000
x + d*b = [2,020446, 1,010252]
***************************************
Iteration 415
Gradient at x = (2,020446, 1,010252) is: (0,000080, -0,000229).
Optimal stepsize is between: [0,101559, 0,101630]
f(x + d*a) = 0,000000
x + d*a = [2,020454, 1,010229]
f(x + d*b) = 0,000000
x + d*b = [2,020454, 1,010229]
***************************************
Iteration 416
Gradient at x = (2,020454, 1,010229) is: (-0,000029, -0,000010).
Optimal stepsize is between: [6,479795, 6,479866]
f(x + d*a) = 0,000000
x + d*a = [2,020266, 1,010162]
f(x + d*b) = 0,000000
x + d*b = [2,020266, 1,010162]
***************************************
Iteration 417
Gradient at x = (2,020266, 1,010162) is: (0,000081, -0,000229).
Optimal stepsize is between: [0,101489, 0,101532]
f(x + d*a) = 0,000000
x + d*a = [2,020274, 1,010138]
f(x + d*b) = 0,000000
x + d*b = [2,020274, 1,010138]
***************************************
Iteration 418
Gradient at x = (2,020274, 1,010138) is: (-0,000028, -0,000010).
Optimal stepsize is between: [6,712740, 6,712811]
f(x + d*a) = 0,000000
x + d*a = [2,020084, 1,010071]
f(x + d*b) = 0,000000
x + d*b = [2,020084, 1,010071]
***************************************
Iteration 419
Gradient at x = (2,020084, 1,010071) is: (0,000081, -0,000227).
Optimal stepsize is between: [0,101418, 0,101489]
f(x + d*a) = 0,000000
x + d*a = [2,020093, 1,010048]
f(x + d*b) = 0,000000
x + d*b = [2,020093, 1,010048]
***************************************
Iteration 420
Gradient at x = (2,020093, 1,010048) is: (-0,000027, -0,000010).
Optimal stepsize is between: [7,118140, 7,118211]
f(x + d*a) = 0,000000
x + d*a = [2,019897, 1,009977]
f(x + d*b) = 0,000000
x + d*b = [2,019897, 1,009977]
***************************************
Iteration 421
Gradient at x = (2,019897, 1,009977) is: (0,000082, -0,000227).
Optimal stepsize is between: [0,101347, 0,101391]
f(x + d*a) = 0,000000
x + d*a = [2,019905, 1,009954]
f(x + d*b) = 0,000000
x + d*b = [2,019905, 1,009954]
***************************************
Iteration 422
Gradient at x = (2,019905, 1,009954) is: (-0,000027, -0,000010).
Optimal stepsize is between: [7,409425, 7,409496]
f(x + d*a) = 0,000000
x + d*a = [2,019708, 1,009882]
f(x + d*b) = 0,000000
x + d*b = [2,019708, 1,009882]
***************************************
Iteration 423
Gradient at x = (2,019708, 1,009882) is: (0,000082, -0,000225).
Optimal stepsize is between: [0,101304, 0,101347]
f(x + d*a) = 0,000000
x + d*a = [2,019716, 1,009859]
f(x + d*b) = 0,000000
x + d*b = [2,019716, 1,009859]
***************************************
Iteration 424
Gradient at x = (2,019716, 1,009859) is: (-0,000026, -0,000009).
Optimal stepsize is between: [7,589705, 7,589748]
f(x + d*a) = 0,000000
x + d*a = [2,019519, 1,009787]
f(x + d*b) = 0,000000
x + d*b = [2,019519, 1,009787]
***************************************
Iteration 425
Gradient at x = (2,019519, 1,009787) is: (0,000081, -0,000221).
Optimal stepsize is between: [0,101260, 0,101331]
f(x + d*a) = 0,000000
x + d*a = [2,019527, 1,009765]
f(x + d*b) = 0,000000
x + d*b = [2,019527, 1,009765]
***************************************
Iteration 426
Gradient at x = (2,019527, 1,009765) is: (-0,000025, -0,000009).
Optimal stepsize is between: [7,917416, 7,917486]
f(x + d*a) = 0,000000
x + d*a = [2,019328, 1,009692]
f(x + d*b) = 0,000000
x + d*b = [2,019328, 1,009692]
***************************************
Iteration 427
Gradient at x = (2,019328, 1,009692) is: (0,000081, -0,000219).
Optimal stepsize is between: [0,101189, 0,101260]
f(x + d*a) = 0,000000
x + d*a = [2,019336, 1,009669]
f(x + d*b) = 0,000000
x + d*b = [2,019336, 1,009669]
***************************************
Iteration 428
Gradient at x = (2,019336, 1,009669) is: (-0,000024, -0,000009).
Optimal stepsize is between: [8,484318, 8,484362]
f(x + d*a) = 0,000000
x + d*a = [2,019130, 1,009592]
f(x + d*b) = 0,000000
x + d*b = [2,019130, 1,009592]
***************************************
Iteration 429
Gradient at x = (2,019130, 1,009592) is: (0,000082, -0,000220).
Optimal stepsize is between: [0,101119, 0,101162]
f(x + d*a) = 0,000000
x + d*a = [2,019138, 1,009570]
f(x + d*b) = 0,000000
x + d*b = [2,019138, 1,009570]
***************************************
Iteration 430
Gradient at x = (2,019138, 1,009570) is: (-0,000024, -0,000009).
Optimal stepsize is between: [8,910831, 8,910902]
f(x + d*a) = 0,000000
x + d*a = [2,018928, 1,009491]
f(x + d*b) = 0,000000
x + d*b = [2,018928, 1,009491]
***************************************
Iteration 431
Gradient at x = (2,018928, 1,009491) is: (0,000082, -0,000218).
Optimal stepsize is between: [0,101048, 0,101119]
f(x + d*a) = 0,000000
x + d*a = [2,018936, 1,009469]
f(x + d*b) = 0,000000
x + d*b = [2,018936, 1,009469]
***************************************
Iteration 432
Gradient at x = (2,018936, 1,009469) is: (-0,000023, -0,000009).
Optimal stepsize is between: [9,621887, 9,621957]
f(x + d*a) = 0,000000
x + d*a = [2,018716, 1,009385]
f(x + d*b) = 0,000000
x + d*b = [2,018716, 1,009385]
***************************************
Iteration 433
Gradient at x = (2,018716, 1,009385) is: (0,000083, -0,000219).
Optimal stepsize is between: [0,100960, 0,101031]
f(x + d*a) = 0,000000
x + d*a = [2,018725, 1,009363]
f(x + d*b) = 0,000000
x + d*b = [2,018725, 1,009363]
***************************************
Iteration 434
Gradient at x = (2,018725, 1,009363) is: (-0,000022, -0,000008).
Optimal stepsize is between: [10,532810, 10,532854]
f(x + d*a) = 0,000000
x + d*a = [2,018493, 1,009274]
f(x + d*b) = 0,000000
x + d*b = [2,018493, 1,009274]
***************************************
Iteration 435
Gradient at x = (2,018493, 1,009274) is: (0,000085, -0,000221).
Optimal stepsize is between: [0,100890, 0,100960]
f(x + d*a) = 0,000000
x + d*a = [2,018501, 1,009252]
f(x + d*b) = 0,000000
x + d*b = [2,018501, 1,009252]
***************************************
Iteration 436
Gradient at x = (2,018501, 1,009252) is: (-0,000021, -0,000008).
Optimal stepsize is between: [11,146149, 11,146219]
f(x + d*a) = 0,000000
x + d*a = [2,018265, 1,009160]
f(x + d*b) = 0,000000
x + d*b = [2,018265, 1,009160]
***************************************
Iteration 437
Gradient at x = (2,018265, 1,009160) is: (0,000085, -0,000219).
Optimal stepsize is between: [0,100819, 0,100890]
f(x + d*a) = 0,000000
x + d*a = [2,018273, 1,009138]
f(x + d*b) = 0,000000
x + d*b = [2,018273, 1,009138]
***************************************
Iteration 438
Gradient at x = (2,018273, 1,009138) is: (-0,000020, -0,000008).
Optimal stepsize is between: [12,347687, 12,347758]
f(x + d*a) = 0,000000
x + d*a = [2,018022, 1,009039]
f(x + d*b) = 0,000000
x + d*b = [2,018022, 1,009039]
***************************************
Iteration 439
Gradient at x = (2,018022, 1,009039) is: (0,000087, -0,000221).
Optimal stepsize is between: [0,100748, 0,100792]
f(x + d*a) = 0,000000
x + d*a = [2,018031, 1,009016]
f(x + d*b) = 0,000000
x + d*b = [2,018031, 1,009016]
***************************************
Iteration 440
Gradient at x = (2,018031, 1,009016) is: (-0,000020, -0,000008).
Optimal stepsize is between: [13,252751, 13,252822]
f(x + d*a) = 0,000000
x + d*a = [2,017771, 1,008913]
f(x + d*b) = 0,000000
x + d*b = [2,017771, 1,008913]
***************************************
Iteration 441
Gradient at x = (2,017771, 1,008913) is: (0,000087, -0,000220).
Optimal stepsize is between: [0,100705, 0,100748]
f(x + d*a) = 0,000000
x + d*a = [2,017780, 1,008891]
f(x + d*b) = 0,000000
x + d*b = [2,017780, 1,008891]
***************************************
Iteration 442
Gradient at x = (2,017780, 1,008891) is: (-0,000019, -0,000007).
Optimal stepsize is between: [13,870038, 13,870081]
f(x + d*a) = 0,000000
x + d*a = [2,017520, 1,008787]
f(x + d*b) = 0,000000
x + d*b = [2,017520, 1,008787]
***************************************
Iteration 443
Gradient at x = (2,017520, 1,008787) is: (0,000086, -0,000215).
Optimal stepsize is between: [0,100661, 0,100732]
f(x + d*a) = 0,000000
x + d*a = [2,017529, 1,008765]
f(x + d*b) = 0,000000
x + d*b = [2,017529, 1,008765]
***************************************
Iteration 444
Gradient at x = (2,017529, 1,008765) is: (-0,000018, -0,000007).
Optimal stepsize is between: [14,995130, 14,995200]
f(x + d*a) = 0,000000
x + d*a = [2,017260, 1,008657]
f(x + d*b) = 0,000000
x + d*b = [2,017260, 1,008657]
***************************************
Iteration 445
Gradient at x = (2,017260, 1,008657) is: (0,000086, -0,000214).
Optimal stepsize is between: [0,100590, 0,100661]
f(x + d*a) = 0,000000
x + d*a = [2,017269, 1,008635]
f(x + d*b) = 0,000000
x + d*b = [2,017269, 1,008635]
***************************************
Iteration 446
Gradient at x = (2,017269, 1,008635) is: (-0,000017, -0,000007).
Optimal stepsize is between: [17,209873, 17,209917]
f(x + d*a) = 0,000000
x + d*a = [2,016975, 1,008515]
f(x + d*b) = 0,000000
x + d*b = [2,016975, 1,008515]
***************************************
Iteration 447
Gradient at x = (2,016975, 1,008515) is: (0,000089, -0,000218).
Optimal stepsize is between: [0,100519, 0,100563]
f(x + d*a) = 0,000000
x + d*a = [2,016984, 1,008493]
f(x + d*b) = 0,000000
x + d*b = [2,016984, 1,008493]
***************************************
Iteration 448
Gradient at x = (2,016984, 1,008493) is: (-0,000016, -0,000007).
Optimal stepsize is between: [19,112600, 19,112671]
f(x + d*a) = 0,000000
x + d*a = [2,016673, 1,008364]
f(x + d*b) = 0,000000
x + d*b = [2,016673, 1,008364]
***************************************
Iteration 449
Gradient at x = (2,016673, 1,008364) is: (0,000090, -0,000218).
Optimal stepsize is between: [0,100449, 0,100519]
f(x + d*a) = 0,000000
x + d*a = [2,016683, 1,008342]
f(x + d*b) = 0,000000
x + d*b = [2,016683, 1,008342]
***************************************
Iteration 450
Gradient at x = (2,016683, 1,008342) is: (-0,000015, -0,000006).
Optimal stepsize is between: [22,694306, 22,694377]
f(x + d*a) = 0,000000
x + d*a = [2,016335, 1,008195]
f(x + d*b) = 0,000000
x + d*b = [2,016335, 1,008195]
***************************************
Iteration 451
Gradient at x = (2,016335, 1,008195) is: (0,000094, -0,000224).
Optimal stepsize is between: [0,100378, 0,100422]
f(x + d*a) = 0,000000
x + d*a = [2,016344, 1,008173]
f(x + d*b) = 0,000000
x + d*b = [2,016344, 1,008173]
***************************************
Iteration 452
Gradient at x = (2,016344, 1,008173) is: (-0,000014, -0,000006).
Optimal stepsize is between: [26,383881, 26,383952]
f(x + d*a) = 0,000000
x + d*a = [2,015964, 1,008010]
f(x + d*b) = 0,000000
x + d*b = [2,015964, 1,008010]
***************************************
Iteration 453
Gradient at x = (2,015964, 1,008010) is: (0,000096, -0,000225).
Optimal stepsize is between: [0,100334, 0,100378]
f(x + d*a) = 0,000000
x + d*a = [2,015974, 1,007988]
f(x + d*b) = 0,000000
x + d*b = [2,015974, 1,007988]
***************************************
Iteration 454
Gradient at x = (2,015974, 1,007988) is: (-0,000013, -0,000006).
Optimal stepsize is between: [28,632002, 28,632045]
f(x + d*a) = 0,000000
x + d*a = [2,015590, 1,007822]
f(x + d*b) = 0,000000
x + d*b = [2,015590, 1,007822]
***************************************
Iteration 455
Gradient at x = (2,015590, 1,007822) is: (0,000094, -0,000219).
Optimal stepsize is between: [0,100291, 0,100361]
f(x + d*a) = 0,000000
x + d*a = [2,015599, 1,007800]
f(x + d*b) = 0,000000
x + d*b = [2,015599, 1,007800]
***************************************
Iteration 456
Gradient at x = (2,015599, 1,007800) is: (-0,000012, -0,000005).
Optimal stepsize is between: [34,321236, 34,321307]
f(x + d*a) = 0,000000
x + d*a = [2,015172, 1,007614]
f(x + d*b) = 0,000000
x + d*b = [2,015172, 1,007614]
***************************************
Iteration 457
Gradient at x = (2,015172, 1,007614) is: (0,000097, -0,000221).
Optimal stepsize is between: [0,100220, 0,100291]
f(x + d*a) = 0,000000
x + d*a = [2,015181, 1,007591]
f(x + d*b) = 0,000000
x + d*b = [2,015181, 1,007591]
***************************************
Iteration 458
Gradient at x = (2,015181, 1,007591) is: (-0,000011, -0,000005).
Optimal stepsize is between: [47,678156, 47,678226]
f(x + d*a) = 0,000000
x + d*a = [2,014636, 1,007348]
f(x + d*b) = 0,000000
x + d*b = [2,014636, 1,007348]
***************************************
Iteration 459
Gradient at x = (2,014636, 1,007348) is: (0,000106, -0,000236).
Optimal stepsize is between: [0,100149, 0,100193]
f(x + d*a) = 0,000000
x + d*a = [2,014647, 1,007324]
f(x + d*b) = 0,000000
x + d*b = [2,014647, 1,007324]
***************************************
Iteration 460
Gradient at x = (2,014647, 1,007324) is: (-0,000010, -0,000005).
Optimal stepsize is between: [68,618956, 68,619026]
f(x + d*a) = 0,000000
x + d*a = [2,013944, 1,007003]
f(x + d*b) = 0,000000
x + d*b = [2,013944, 1,007003]
***************************************
Iteration 461
Gradient at x = (2,013944, 1,007003) is: (0,000114, -0,000249).
Optimal stepsize is between: [0,100079, 0,100149]
f(x + d*a) = 0,000000
x + d*a = [2,013956, 1,006978]
f(x + d*b) = 0,000000
x + d*b = [2,013956, 1,006978]
***************************************
Iteration 462
Gradient at x = (2,013956, 1,006978) is: (-0,000009, -0,000004).
Optimal stepsize is between: [139,140989, 139,141060]
f(x + d*a) = 0,000000
x + d*a = [2,012731, 1,006402]
f(x + d*b) = 0,000000
x + d*b = [2,012731, 1,006402]
***************************************
Iteration 463
Gradient at x = (2,012731, 1,006402) is: (0,000135, -0,000286).
Optimal stepsize is between: [0,100035, 0,100079]
f(x + d*a) = 0,000000
x + d*a = [2,012745, 1,006373]
f(x + d*b) = 0,000000
x + d*b = [2,012745, 1,006373]
***************************************
Iteration 464
Gradient at x = (2,012745, 1,006373) is: (-0,000007, -0,000003).
Optimal stepsize is between: [243,791539, 243,791610]
f(x + d*a) = 0,000000
x + d*a = [2,011117, 1,005592]
f(x + d*b) = 0,000000
x + d*b = [2,011117, 1,005592]
***************************************
Iteration 465
Gradient at x = (2,011117, 1,005592) is: (0,000129, -0,000269).
Optimal stepsize is between: [0,099991, 0,100062]
f(x + d*a) = 0,000000
x + d*a = [2,011130, 1,005565]
f(x + d*b) = 0,000000
x + d*b = [2,011130, 1,005565]
***************************************
Iteration 466
Gradient at x = (2,011130, 1,005565) is: (-0,000004, -0,000002).
Optimal stepsize is between: [1173,321368, 1173,321439]
f(x + d*a) = 0,000000
x + d*a = [2,005963, 1,002958]
f(x + d*b) = 0,000000
x + d*b = [2,005963, 1,002958]
***************************************
Iteration 467
Gradient at x = (2,005963, 1,002958) is: (-0,000094, 0,000186).
Optimal stepsize is between: [0,099964, 0,100035]
f(x + d*a) = 0,000000
x + d*a = [2,005953, 1,002977]
f(x + d*b) = 0,000000
x + d*b = [2,005953, 1,002977]
***************************************
Iteration 468
Gradient at x = (2,005953, 1,002977) is: (-0,000001, -0,000000).
Therefore, the solver procedure is stopped.
The procedure stopped at step 468
------------------------------------------------------------------------
BUILD SUCCESS
------------------------------------------------------------------------
Total time: 3.614 s
Finished at: 2020-02-25T23:11:53+03:00
Final Memory: 11M/40M
------------------------------------------------------------------------
